{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHDiTjr5lm_U"
   },
   "source": [
    "# PROJECT OVERVIEW\n",
    "\n",
    "James M. Irving, Ph.D.\n",
    "\n",
    "Flatiron Full Time Data Science 021119 Cohort\n",
    "\n",
    "https://mybinder.org/v2/gh/jirvingphd/bs_ds/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:10.957221Z",
     "start_time": "2020-07-12T01:04:09.370662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.22 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_98194ffa_c3db_11ea_8074_4865ee12e626row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x112a56668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U cufflinks\n",
    "# !pip install -U fsds\n",
    "\n",
    "# %conda config --add channels conda-forge\n",
    "# %conda install qgrid\n",
    "\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:10.966849Z",
     "start_time": "2020-07-12T01:04:10.958821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## IMPORT CONVENIENCE/DISPLAY FUNCTIONS\n",
    "from pprint import pprint\n",
    "import qgrid\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Import plotly and cufflinks for iplots\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:10.975375Z",
     "start_time": "2020-07-12T01:04:10.968706Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('py_files/')\n",
    "\n",
    "## IMPORT MY PUBLISHED PYPI PACKAGE \n",
    "# import bs_ds as  bs\n",
    "import bs_ds_local as bs\n",
    "# from fsds_100719.imports import *\n",
    "# from bs_ds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:10.991051Z",
     "start_time": "2020-07-12T01:04:10.977121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## IMPORT CUSTOM CAPSTONE FUNCTIONS\n",
    "import functions_combined_BEST as ji\n",
    "import functions_io as io\n",
    "\n",
    "from functions_combined_BEST import ihelp, ihelp_menu,\\\n",
    "reload, inspect_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:10.995994Z",
     "start_time": "2020-07-12T01:04:10.992970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set pd.set_options for tweet visibility\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.max_columns',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:11.034402Z",
     "start_time": "2020-07-12T01:04:10.997599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Dictionary Contents ------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e23b1534e44f21892cba7f1ba6accb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dict_to_display', options={'file_directory': {'history': '', 'file…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] filename_directory saved to data/filename_dictionary.json.\n",
      "\t - use `update_file_directory(file_dict)` to update file.\n",
      "[i] creating all required folders...\n"
     ]
    }
   ],
   "source": [
    "## Saving the sys.stdout to restore later\n",
    "import sys\n",
    "__stdout__=sys.stdout\n",
    "\n",
    "file_dict = io.def_filename_dictionary(load_prior=False, save_directory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "### DATA SOURCES:\n",
    "\n",
    "* **All Donald Trump tweets from 12/01/2016 (pre-inaugaration day) to end of 08/23/2018**\n",
    "    *          Extracted from http://www.trumptwitterarchive.com/\n",
    "\n",
    "* **Minute-resolution data for the S&P500 covering the same time period.**\n",
    "    *         IVE S&P500 Index from - http://www.kibot.com/free_historical_data.aspx\n",
    "    - (***Aggregate 1 min bid-ask data link***)\n",
    "        - http://api.kibot.com/?action=history&symbol=IVE&interval=tickbidask1&bp=1&user=guest\n",
    "    \n",
    "    \n",
    "* NOTE: Both sources required manual extraction and both 1-min historical stock data and batch-historical-tweet data are difficult to obtain without paying \\\\$150-\\\\$2000 monthly developer memberships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:11.205079Z",
     "start_time": "2020-07-12T01:04:11.035859Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Attempt to Get Stock Data With Code\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def download_stock_data(fpath='data/ive_minute_tick_bidask_API.csv',\n",
    "                       verbose=True):\n",
    "    \"\"\"Downloads up-to-date IVE S&P 500 1-min aggregate data from \n",
    "    http://www.kibot.com/free_historical_data.aspx\n",
    "    \n",
    "    Args:\n",
    "        fpath (str): csv filepath to save (Default='data/ive_minute_tick_bidask_API.csv')\n",
    "        verbose (bool): Display file info (Default=True)\n",
    "        \n",
    "    Returns:\n",
    "        stock_df: DataFrame with correct headers and datetime index\"\"\"\n",
    "    agg_url = 'http://api.kibot.com/?action=history&symbol=IVE&interval=tickbidask1&bp=1&user=guest'\n",
    "    response = requests.get(agg_url,\n",
    "                            allow_redirects=True)\n",
    "\n",
    "    ## Save output to csv file\n",
    "    with open(fpath,'wb') as file:\n",
    "        file.write(response.content)\n",
    "        \n",
    "        \n",
    "    ## Load in Stock Data Frame with headers (then save)\n",
    "    headers = ['Date','Time','BidOpen','BidHigh','BidLow','BidClose','AskOpen','AskHigh','AskLow','AskClose']\n",
    "    stock_df = pd.read_csv('data/ive_minute_tick_bidask_API.csv',names=headers)\n",
    "\n",
    "# \n",
    "    ## Make Combined Date Time column and Drop Origs\n",
    "    stock_df['datetime'] = pd.to_datetime(stock_df['Date'].astype(str)+' '+stock_df['Time'].astype(str))\n",
    "    stock_df.to_csv(fpath,index=False)\n",
    "        \n",
    "    if verbose:\n",
    "        print('[i] Data successfully downloaded and saved as:')\n",
    "        print(' - ',fpath)\n",
    "        \n",
    "    return pd.read_csv(fpath,parse_dates=['datetime'],index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.317301Z",
     "start_time": "2020-07-12T01:04:11.207583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1442a9160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stock_df = download_stock_data_csv()\n",
    "stock_df = pd.read_csv('data/ive_minute_tick_bidask_API.csv',\n",
    "                       parse_dates=['datetime'],\n",
    "                       index_col='datetime')\n",
    "\n",
    "## Remove Other Columns\n",
    "stock_df = stock_df[['BidOpen','BidClose']]\n",
    "\n",
    "\n",
    "## Remove rare 0-values\n",
    "stock_df = stock_df[(stock_df>0).all(axis=1)]\n",
    "\n",
    "\n",
    "stock_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.327388Z",
     "start_time": "2020-07-12T01:04:15.319324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BidOpen</th>\n",
       "      <th>BidClose</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-09-28 09:30:00</th>\n",
       "      <td>39.35</td>\n",
       "      <td>39.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-28 09:31:00</th>\n",
       "      <td>39.38</td>\n",
       "      <td>39.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-28 09:32:00</th>\n",
       "      <td>39.39</td>\n",
       "      <td>39.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-28 09:33:00</th>\n",
       "      <td>39.42</td>\n",
       "      <td>39.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-28 09:34:00</th>\n",
       "      <td>39.42</td>\n",
       "      <td>39.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     BidOpen  BidClose\n",
       "datetime                              \n",
       "2009-09-28 09:30:00    39.35     39.35\n",
       "2009-09-28 09:31:00    39.38     39.38\n",
       "2009-09-28 09:32:00    39.39     39.43\n",
       "2009-09-28 09:33:00    39.42     39.42\n",
       "2009-09-28 09:34:00    39.42     39.41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.348030Z",
     "start_time": "2020-07-12T01:04:15.329079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2009-09-28 09:30:00', '2009-09-28 09:31:00',\n",
       "               '2009-09-28 09:32:00', '2009-09-28 09:33:00',\n",
       "               '2009-09-28 09:34:00', '2009-09-28 09:36:00',\n",
       "               '2009-09-28 09:37:00', '2009-09-28 09:38:00',\n",
       "               '2009-09-28 09:40:00', '2009-09-28 09:41:00',\n",
       "               ...\n",
       "               '2020-07-10 15:50:00', '2020-07-10 15:51:00',\n",
       "               '2020-07-10 15:52:00', '2020-07-10 15:53:00',\n",
       "               '2020-07-10 15:54:00', '2020-07-10 15:55:00',\n",
       "               '2020-07-10 15:56:00', '2020-07-10 15:58:00',\n",
       "               '2020-07-10 15:59:00', '2020-07-10 16:00:00'],\n",
       "              dtype='datetime64[ns]', name='datetime', length=888284, freq=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.352264Z",
     "start_time": "2020-07-12T01:04:15.349691Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## Load in Stock Data Frame with headers (then save)\n",
    "# headers = ['Date','Time','BidOpen','BidHigh','BidLow','BidClose','AskOpen','AskHigh','AskLow','AskClose']\n",
    "# stock_df = pd.read_csv('data/ive_minute_tick_bidask_API.csv',names=headers)\n",
    "\n",
    "# ## Convert Date and Time for Making Date-Time\n",
    "# convert_cols = ['Date','Time']\n",
    "# for col in convert_cols:\n",
    "#     stock_df[col] = stock_df[col].astype(str)\n",
    "\n",
    "# ## Make Combined Date Time column and Drop Origs\n",
    "# stock_df['datetime'] = pd.to_datetime(stock_df['Date']+' '+stock_df['Time'])\n",
    "# stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.355937Z",
     "start_time": "2020-07-12T01:04:15.353883Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # fullfilename = '/Users/jamesirving/Downloads/IVE_tickbidask (4).txt'\n",
    "# fullfilename = '/Users/jamesirving/Downloads/IVE_bidask1min (2).txt'\n",
    "# headers = ['Date','Time','BidOpen','BidHigh','BidLow','BidClose','AskOpen','AskHigh','AskLow','AskClose']\n",
    "# stock_df = pd.read_csv(fullfilename, names=headers,parse_dates=True)\n",
    "# stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tweet Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.359531Z",
     "start_time": "2020-07-12T01:04:15.357341Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# reload(ji)\n",
    "# func_list = [ji.load_raw_twitter_file,\n",
    "#            ji.make_stopwords_list,\n",
    "#            ji.full_twitter_df_processing,\n",
    "#            ji.full_sentiment_analysis]\n",
    "# ji.ihelp_menu(func_list)\n",
    "# # ji.save_ihelp_menu_to_file(func_list,filename='_twitter_processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:15.362989Z",
     "start_time": "2020-07-12T01:04:15.360813Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:17.535213Z",
     "start_time": "2020-07-12T01:04:15.364332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] Loading raw tweet text file: data/trump_twitter_archive_07112020.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:44:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...</td>\n",
       "      <td>2020-07-11 18:44:28</td>\n",
       "      <td>4728</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022963987054592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...</td>\n",
       "      <td>2020-07-11 18:43:57</td>\n",
       "      <td>8574</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022835867791361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...</td>\n",
       "      <td>2020-07-11 18:43:47</td>\n",
       "      <td>6188</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022791857025025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:42:55</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...</td>\n",
       "      <td>2020-07-11 18:42:55</td>\n",
       "      <td>3612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022576441700356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1281940302782136320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 source  \\\n",
       "date                                      \n",
       "2020-07-11 18:44:28  Twitter for iPhone   \n",
       "2020-07-11 18:43:57  Twitter for iPhone   \n",
       "2020-07-11 18:43:47  Twitter for iPhone   \n",
       "2020-07-11 18:42:55  Twitter for iPhone   \n",
       "2020-07-11 13:16:00  Twitter for iPhone   \n",
       "\n",
       "                                                                                                                 content  \\\n",
       "date                                                                                                                       \n",
       "2020-07-11 18:44:28  RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...   \n",
       "2020-07-11 18:43:57  RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...   \n",
       "2020-07-11 18:43:47  RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...   \n",
       "2020-07-11 18:42:55  RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...   \n",
       "2020-07-11 13:16:00      RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!   \n",
       "\n",
       "                                   date  retweet_count  favorite_count  \\\n",
       "date                                                                     \n",
       "2020-07-11 18:44:28 2020-07-11 18:44:28           4728               0   \n",
       "2020-07-11 18:43:57 2020-07-11 18:43:57           8574               0   \n",
       "2020-07-11 18:43:47 2020-07-11 18:43:47           6188               0   \n",
       "2020-07-11 18:42:55 2020-07-11 18:42:55           3612               0   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00          39286               0   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "date                                                 \n",
       "2020-07-11 18:44:28       True  1282022963987054592  \n",
       "2020-07-11 18:43:57       True  1282022835867791361  \n",
       "2020-07-11 18:43:47       True  1282022791857025025  \n",
       "2020-07-11 18:42:55       True  1282022576441700356  \n",
       "2020-07-11 13:16:00       True  1281940302782136320  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from functions_combined_BEST import ihelp_menu2\n",
    "# raw_tweets = 'data/trump_tweets_12012016_to_01012020.csv'\n",
    "raw_tweets = 'data/trump_twitter_archive_07112020.csv'\n",
    "\n",
    "## Load in raw csv of twitter_data, create date_time_index, rename columns\n",
    "# raw_tweets ='data/trump_tweets_12012016_to_01012020.csv'\n",
    "twitter_df = ji.load_raw_twitter_file(filename=raw_tweets, \n",
    "                         date_as_index=True,\n",
    "                         rename_map={'text': 'content',\n",
    "                                     'created_at': 'date'})\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07/11/20 - Changing ORder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import stock data FIRST and get delta-tweet price change\n",
    "2. Use Sklearn RandomForest with TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:17.542260Z",
     "start_time": "2020-07-12T01:04:17.536568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "------ SOURCE ----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def match_stock_price_to_tweets(tweet_timestamp,time_after_tweet= 60,time_freq ='T',stock_price=[]):#stock_price_index=stock_date_data):\n",
       "    \n",
       "    import pandas as pd\n",
       "    import numpy as np\n",
       "    from datetime import datetime as dt\n",
       "    # output={'pre_tweet_price': price_at_tweet,'post_tweet_price':price_after_tweet,'delta_price':delta_price, 'delta_time':delta_time}\n",
       "    output={}\n",
       "    # convert tweet timestamp to minute accuracy\n",
       "    ts=[]\n",
       "    ts = pd.to_datetime(tweet_timestamp).round(time_freq)\n",
       "    \n",
       "    BH = pd.tseries.offsets.BusinessHour(start='09:30',end='16:30')\n",
       "    BD = pd.tseries.offsets.BusinessDay()\n",
       "    \n",
       "    \n",
       "    # checking if time is within stock_date_data\n",
       "#     def roll_B_day_forward(ts):\n",
       "     \n",
       "    if ts not in stock_price.index:\n",
       "        ts = BH.rollforward(ts)   \n",
       "\n",
       "        \n",
       "        if ts not in stock_price.index:\n",
       "            return np.nan#\"ts2_not_in_index\"\n",
       "\n",
       "    # Get price at tweet time\n",
       "    price_at_tweet = stock_price.loc[ts]\n",
       "    output['B_ts_rounded'] = ts\n",
       "\n",
       "\n",
       "    if np.isnan(price_at_tweet):\n",
       "        output['pre_tweet_price'] = np.nan\n",
       "    else: \n",
       "        output['pre_tweet_price'] = price_at_tweet\n",
       "\n",
       "    output['mins_after_tweet'] = time_after_tweet\n",
       "               \n",
       "        \n",
       "    # Use timedelta to get desired timepoint following tweet\n",
       "    hour_freqs = 'BH','H','CBH'\n",
       "    day_freqs = 'B','D'\n",
       "\n",
       "    if time_freq=='T':\n",
       "        ofst=pd.offsets.Minute(time_after_tweet)\n",
       "\n",
       "    elif time_freq in hour_freqs:\n",
       "        ofst=pd.offsets.Hour(time_after_tweet)\n",
       "\n",
       "    elif time_freq in day_freqs:\n",
       "        ofst=pd.offsets.Day(time_after_tweet)\n",
       "\n",
       "\n",
       "    # get timestamp to check post-tweet price\n",
       "    post_tweet_ts = ofst(ts)\n",
       "\n",
       "    \n",
       "    if post_tweet_ts not in stock_price.index:\n",
       "#         post_tweet_ts =BD.rollforward(post_tweet_ts)\n",
       "        post_tweet_ts = BH.rollforward(post_tweet_ts)\n",
       "    \n",
       "        if post_tweet_ts not in stock_price.index:\n",
       "            return np.nan\n",
       "\n",
       "    output['B_ts_post_tweet'] = post_tweet_ts\n",
       "\n",
       "    # Get next available stock price\n",
       "    price_after_tweet = stock_price.loc[post_tweet_ts]\n",
       "    if np.isnan(price_after_tweet):\n",
       "        output['post_tweet_price'] = 'NaN in stock_price'\n",
       "    else:\n",
       "        # calculate change in price\n",
       "        delta_price = price_after_tweet - price_at_tweet\n",
       "        delta_time = post_tweet_ts - ts\n",
       "        output['post_tweet_price'] = price_after_tweet\n",
       "        output['delta_time'] = delta_time\n",
       "        output['delta_price'] = delta_price\n",
       "\n",
       "#         output={'pre_tweet_price': price_at_tweet,'post_tweet_price':price_after_tweet,'delta_price':delta_price, 'delta_time':delta_time}\n",
       "\n",
       "    # reorder_output_cols  = ['B_dt_index','pre_tweet_price','']\n",
       "    # reorder_output \n",
       "    return output\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fs.ihelp(ji.match_stock_price_to_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:17.552279Z",
     "start_time": "2020-07-12T01:04:17.543348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:44:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...</td>\n",
       "      <td>2020-07-11 18:44:28</td>\n",
       "      <td>4728</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022963987054592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...</td>\n",
       "      <td>2020-07-11 18:43:57</td>\n",
       "      <td>8574</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022835867791361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...</td>\n",
       "      <td>2020-07-11 18:43:47</td>\n",
       "      <td>6188</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022791857025025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:42:55</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...</td>\n",
       "      <td>2020-07-11 18:42:55</td>\n",
       "      <td>3612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022576441700356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1281940302782136320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 source  \\\n",
       "date                                      \n",
       "2020-07-11 18:44:28  Twitter for iPhone   \n",
       "2020-07-11 18:43:57  Twitter for iPhone   \n",
       "2020-07-11 18:43:47  Twitter for iPhone   \n",
       "2020-07-11 18:42:55  Twitter for iPhone   \n",
       "2020-07-11 13:16:00  Twitter for iPhone   \n",
       "\n",
       "                                                                                                                 content  \\\n",
       "date                                                                                                                       \n",
       "2020-07-11 18:44:28  RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...   \n",
       "2020-07-11 18:43:57  RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...   \n",
       "2020-07-11 18:43:47  RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...   \n",
       "2020-07-11 18:42:55  RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...   \n",
       "2020-07-11 13:16:00      RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!   \n",
       "\n",
       "                                   date  retweet_count  favorite_count  \\\n",
       "date                                                                     \n",
       "2020-07-11 18:44:28 2020-07-11 18:44:28           4728               0   \n",
       "2020-07-11 18:43:57 2020-07-11 18:43:57           8574               0   \n",
       "2020-07-11 18:43:47 2020-07-11 18:43:47           6188               0   \n",
       "2020-07-11 18:42:55 2020-07-11 18:42:55           3612               0   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00          39286               0   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "date                                                 \n",
       "2020-07-11 18:44:28       True  1282022963987054592  \n",
       "2020-07-11 18:43:57       True  1282022835867791361  \n",
       "2020-07-11 18:43:47       True  1282022791857025025  \n",
       "2020-07-11 18:42:55       True  1282022576441700356  \n",
       "2020-07-11 13:16:00       True  1281940302782136320  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:18.022300Z",
     "start_time": "2020-07-12T01:04:17.553572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2009-09-28 09:30:00', '2009-09-28 09:31:00',\n",
       "               '2009-09-28 09:32:00', '2009-09-28 09:33:00',\n",
       "               '2009-09-28 09:34:00', '2009-09-28 09:35:00',\n",
       "               '2009-09-28 09:36:00', '2009-09-28 09:37:00',\n",
       "               '2009-09-28 09:38:00', '2009-09-28 09:39:00',\n",
       "               ...\n",
       "               '2020-07-10 15:51:00', '2020-07-10 15:52:00',\n",
       "               '2020-07-10 15:53:00', '2020-07-10 15:54:00',\n",
       "               '2020-07-10 15:55:00', '2020-07-10 15:56:00',\n",
       "               '2020-07-10 15:57:00', '2020-07-10 15:58:00',\n",
       "               '2020-07-10 15:59:00', '2020-07-10 16:00:00'],\n",
       "              dtype='datetime64[ns]', name='datetime', length=5671111, freq='T')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make 1-min freq stock time series\n",
    "stock_ts = stock_df.asfreq('min')\n",
    "stock_ts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:18.048307Z",
     "start_time": "2020-07-12T01:04:18.024018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>tweet_minute</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:44:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...</td>\n",
       "      <td>2020-07-11 18:44:28</td>\n",
       "      <td>4728</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022963987054592</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...</td>\n",
       "      <td>2020-07-11 18:43:57</td>\n",
       "      <td>8574</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022835867791361</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...</td>\n",
       "      <td>2020-07-11 18:43:47</td>\n",
       "      <td>6188</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022791857025025</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:42:55</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...</td>\n",
       "      <td>2020-07-11 18:42:55</td>\n",
       "      <td>3612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022576441700356</td>\n",
       "      <td>2020-07-11 18:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1281940302782136320</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 23:00:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Well the year has officially begun. I have many stops planned and will be working very hard to w...</td>\n",
       "      <td>2016-01-01 23:00:09</td>\n",
       "      <td>2642</td>\n",
       "      <td>8495</td>\n",
       "      <td>False</td>\n",
       "      <td>683060169677344768</td>\n",
       "      <td>2016-01-01 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 21:29:56</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@sprinklermanus: @CNN @realDonaldTrump they're spending millions but you're still going to win -...</td>\n",
       "      <td>2016-01-01 21:29:56</td>\n",
       "      <td>933</td>\n",
       "      <td>3330</td>\n",
       "      <td>False</td>\n",
       "      <td>683037464504745985</td>\n",
       "      <td>2016-01-01 21:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 06:08:06</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@jallenaip: Hillary said she was in a Fog of War\" as explanation for the lies about Benghazi. No...</td>\n",
       "      <td>2016-01-01 06:08:06</td>\n",
       "      <td>2721</td>\n",
       "      <td>7490</td>\n",
       "      <td>False</td>\n",
       "      <td>682805477168779264</td>\n",
       "      <td>2016-01-01 06:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 06:07:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy New Year from #MarALago! Thank you to my great family for all of their support. https://t....</td>\n",
       "      <td>2016-01-01 06:07:28</td>\n",
       "      <td>1948</td>\n",
       "      <td>8258</td>\n",
       "      <td>False</td>\n",
       "      <td>682805320217980929</td>\n",
       "      <td>2016-01-01 06:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 05:18:23</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>#HappyNewYearAmerica! https://t.co/EeQb8PDrUe</td>\n",
       "      <td>2016-01-01 05:18:23</td>\n",
       "      <td>3434</td>\n",
       "      <td>9143</td>\n",
       "      <td>False</td>\n",
       "      <td>682792967736848385</td>\n",
       "      <td>2016-01-01 05:18:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24020 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "date                                       \n",
       "2020-07-11 18:44:28   Twitter for iPhone   \n",
       "2020-07-11 18:43:57   Twitter for iPhone   \n",
       "2020-07-11 18:43:47   Twitter for iPhone   \n",
       "2020-07-11 18:42:55   Twitter for iPhone   \n",
       "2020-07-11 13:16:00   Twitter for iPhone   \n",
       "...                                  ...   \n",
       "2016-01-01 23:00:09  Twitter for Android   \n",
       "2016-01-01 21:29:56  Twitter for Android   \n",
       "2016-01-01 06:08:06  Twitter for Android   \n",
       "2016-01-01 06:07:28   Twitter for iPhone   \n",
       "2016-01-01 05:18:23   Twitter for iPhone   \n",
       "\n",
       "                                                                                                                 content  \\\n",
       "date                                                                                                                       \n",
       "2020-07-11 18:44:28  RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...   \n",
       "2020-07-11 18:43:57  RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...   \n",
       "2020-07-11 18:43:47  RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...   \n",
       "2020-07-11 18:42:55  RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...   \n",
       "2020-07-11 13:16:00      RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!   \n",
       "...                                                                                                                  ...   \n",
       "2016-01-01 23:00:09  Well the year has officially begun. I have many stops planned and will be working very hard to w...   \n",
       "2016-01-01 21:29:56  @sprinklermanus: @CNN @realDonaldTrump they're spending millions but you're still going to win -...   \n",
       "2016-01-01 06:08:06  @jallenaip: Hillary said she was in a Fog of War\" as explanation for the lies about Benghazi. No...   \n",
       "2016-01-01 06:07:28  Happy New Year from #MarALago! Thank you to my great family for all of their support. https://t....   \n",
       "2016-01-01 05:18:23                                                        #HappyNewYearAmerica! https://t.co/EeQb8PDrUe   \n",
       "\n",
       "                                   date  retweet_count  favorite_count  \\\n",
       "date                                                                     \n",
       "2020-07-11 18:44:28 2020-07-11 18:44:28           4728               0   \n",
       "2020-07-11 18:43:57 2020-07-11 18:43:57           8574               0   \n",
       "2020-07-11 18:43:47 2020-07-11 18:43:47           6188               0   \n",
       "2020-07-11 18:42:55 2020-07-11 18:42:55           3612               0   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00          39286               0   \n",
       "...                                 ...            ...             ...   \n",
       "2016-01-01 23:00:09 2016-01-01 23:00:09           2642            8495   \n",
       "2016-01-01 21:29:56 2016-01-01 21:29:56            933            3330   \n",
       "2016-01-01 06:08:06 2016-01-01 06:08:06           2721            7490   \n",
       "2016-01-01 06:07:28 2016-01-01 06:07:28           1948            8258   \n",
       "2016-01-01 05:18:23 2016-01-01 05:18:23           3434            9143   \n",
       "\n",
       "                    is_retweet               id_str        tweet_minute  \n",
       "date                                                                     \n",
       "2020-07-11 18:44:28       True  1282022963987054592 2020-07-11 18:44:00  \n",
       "2020-07-11 18:43:57       True  1282022835867791361 2020-07-11 18:44:00  \n",
       "2020-07-11 18:43:47       True  1282022791857025025 2020-07-11 18:44:00  \n",
       "2020-07-11 18:42:55       True  1282022576441700356 2020-07-11 18:43:00  \n",
       "2020-07-11 13:16:00       True  1281940302782136320 2020-07-11 13:16:00  \n",
       "...                        ...                  ...                 ...  \n",
       "2016-01-01 23:00:09      False   683060169677344768 2016-01-01 23:00:00  \n",
       "2016-01-01 21:29:56      False   683037464504745985 2016-01-01 21:30:00  \n",
       "2016-01-01 06:08:06      False   682805477168779264 2016-01-01 06:08:00  \n",
       "2016-01-01 06:07:28      False   682805320217980929 2016-01-01 06:07:00  \n",
       "2016-01-01 05:18:23      False   682792967736848385 2016-01-01 05:18:00  \n",
       "\n",
       "[24020 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['tweet_minute'] = twitter_df.index.round(\"T\")\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:04:18.056005Z",
     "start_time": "2020-07-12T01:04:18.049772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-07-11 18:44:00', '2020-07-11 18:44:00',\n",
       "               '2020-07-11 18:44:00', '2020-07-11 18:43:00',\n",
       "               '2020-07-11 13:16:00', '2020-07-11 13:15:00',\n",
       "               '2020-07-11 13:15:00', '2020-07-11 13:15:00',\n",
       "               '2020-07-11 13:15:00', '2020-07-11 13:13:00',\n",
       "               ...\n",
       "               '2016-01-01 23:24:00', '2016-01-01 23:10:00',\n",
       "               '2016-01-01 23:08:00', '2016-01-01 23:06:00',\n",
       "               '2016-01-01 23:02:00', '2016-01-01 23:00:00',\n",
       "               '2016-01-01 21:30:00', '2016-01-01 06:08:00',\n",
       "               '2016-01-01 06:07:00', '2016-01-01 05:18:00'],\n",
       "              dtype='datetime64[ns]', name='date', length=24020, freq=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get a Rounded version of the tweet time stamps\n",
    "tweet_ts = twitter_df.index.round('T')\n",
    "tweet_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:05:29.699674Z",
     "start_time": "2020-07-12T01:05:29.678287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>tweet_minute</th>\n",
       "      <th>tweet_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:44:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...</td>\n",
       "      <td>2020-07-11 18:44:28</td>\n",
       "      <td>4728</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022963987054592</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...</td>\n",
       "      <td>2020-07-11 18:43:57</td>\n",
       "      <td>8574</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022835867791361</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...</td>\n",
       "      <td>2020-07-11 18:43:47</td>\n",
       "      <td>6188</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022791857025025</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:42:55</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...</td>\n",
       "      <td>2020-07-11 18:42:55</td>\n",
       "      <td>3612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022576441700356</td>\n",
       "      <td>2020-07-11 18:43:00</td>\n",
       "      <td>05:26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1281940302782136320</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>00:00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 23:00:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Well the year has officially begun. I have many stops planned and will be working very hard to w...</td>\n",
       "      <td>2016-01-01 23:00:09</td>\n",
       "      <td>2642</td>\n",
       "      <td>8495</td>\n",
       "      <td>False</td>\n",
       "      <td>683060169677344768</td>\n",
       "      <td>2016-01-01 23:00:00</td>\n",
       "      <td>01:30:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 21:29:56</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@sprinklermanus: @CNN @realDonaldTrump they're spending millions but you're still going to win -...</td>\n",
       "      <td>2016-01-01 21:29:56</td>\n",
       "      <td>933</td>\n",
       "      <td>3330</td>\n",
       "      <td>False</td>\n",
       "      <td>683037464504745985</td>\n",
       "      <td>2016-01-01 21:30:00</td>\n",
       "      <td>15:21:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 06:08:06</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@jallenaip: Hillary said she was in a Fog of War\" as explanation for the lies about Benghazi. No...</td>\n",
       "      <td>2016-01-01 06:08:06</td>\n",
       "      <td>2721</td>\n",
       "      <td>7490</td>\n",
       "      <td>False</td>\n",
       "      <td>682805477168779264</td>\n",
       "      <td>2016-01-01 06:08:00</td>\n",
       "      <td>00:00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 06:07:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy New Year from #MarALago! Thank you to my great family for all of their support. https://t....</td>\n",
       "      <td>2016-01-01 06:07:28</td>\n",
       "      <td>1948</td>\n",
       "      <td>8258</td>\n",
       "      <td>False</td>\n",
       "      <td>682805320217980929</td>\n",
       "      <td>2016-01-01 06:07:00</td>\n",
       "      <td>00:49:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 05:18:23</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>#HappyNewYearAmerica! https://t.co/EeQb8PDrUe</td>\n",
       "      <td>2016-01-01 05:18:23</td>\n",
       "      <td>3434</td>\n",
       "      <td>9143</td>\n",
       "      <td>False</td>\n",
       "      <td>682792967736848385</td>\n",
       "      <td>2016-01-01 05:18:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24020 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "date                                       \n",
       "2020-07-11 18:44:28   Twitter for iPhone   \n",
       "2020-07-11 18:43:57   Twitter for iPhone   \n",
       "2020-07-11 18:43:47   Twitter for iPhone   \n",
       "2020-07-11 18:42:55   Twitter for iPhone   \n",
       "2020-07-11 13:16:00   Twitter for iPhone   \n",
       "...                                  ...   \n",
       "2016-01-01 23:00:09  Twitter for Android   \n",
       "2016-01-01 21:29:56  Twitter for Android   \n",
       "2016-01-01 06:08:06  Twitter for Android   \n",
       "2016-01-01 06:07:28   Twitter for iPhone   \n",
       "2016-01-01 05:18:23   Twitter for iPhone   \n",
       "\n",
       "                                                                                                                 content  \\\n",
       "date                                                                                                                       \n",
       "2020-07-11 18:44:28  RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...   \n",
       "2020-07-11 18:43:57  RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...   \n",
       "2020-07-11 18:43:47  RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...   \n",
       "2020-07-11 18:42:55  RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...   \n",
       "2020-07-11 13:16:00      RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!   \n",
       "...                                                                                                                  ...   \n",
       "2016-01-01 23:00:09  Well the year has officially begun. I have many stops planned and will be working very hard to w...   \n",
       "2016-01-01 21:29:56  @sprinklermanus: @CNN @realDonaldTrump they're spending millions but you're still going to win -...   \n",
       "2016-01-01 06:08:06  @jallenaip: Hillary said she was in a Fog of War\" as explanation for the lies about Benghazi. No...   \n",
       "2016-01-01 06:07:28  Happy New Year from #MarALago! Thank you to my great family for all of their support. https://t....   \n",
       "2016-01-01 05:18:23                                                        #HappyNewYearAmerica! https://t.co/EeQb8PDrUe   \n",
       "\n",
       "                                   date  retweet_count  favorite_count  \\\n",
       "date                                                                     \n",
       "2020-07-11 18:44:28 2020-07-11 18:44:28           4728               0   \n",
       "2020-07-11 18:43:57 2020-07-11 18:43:57           8574               0   \n",
       "2020-07-11 18:43:47 2020-07-11 18:43:47           6188               0   \n",
       "2020-07-11 18:42:55 2020-07-11 18:42:55           3612               0   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00          39286               0   \n",
       "...                                 ...            ...             ...   \n",
       "2016-01-01 23:00:09 2016-01-01 23:00:09           2642            8495   \n",
       "2016-01-01 21:29:56 2016-01-01 21:29:56            933            3330   \n",
       "2016-01-01 06:08:06 2016-01-01 06:08:06           2721            7490   \n",
       "2016-01-01 06:07:28 2016-01-01 06:07:28           1948            8258   \n",
       "2016-01-01 05:18:23 2016-01-01 05:18:23           3434            9143   \n",
       "\n",
       "                    is_retweet               id_str        tweet_minute  \\\n",
       "date                                                                      \n",
       "2020-07-11 18:44:28       True  1282022963987054592 2020-07-11 18:44:00   \n",
       "2020-07-11 18:43:57       True  1282022835867791361 2020-07-11 18:44:00   \n",
       "2020-07-11 18:43:47       True  1282022791857025025 2020-07-11 18:44:00   \n",
       "2020-07-11 18:42:55       True  1282022576441700356 2020-07-11 18:43:00   \n",
       "2020-07-11 13:16:00       True  1281940302782136320 2020-07-11 13:16:00   \n",
       "...                        ...                  ...                 ...   \n",
       "2016-01-01 23:00:09      False   683060169677344768 2016-01-01 23:00:00   \n",
       "2016-01-01 21:29:56      False   683037464504745985 2016-01-01 21:30:00   \n",
       "2016-01-01 06:08:06      False   682805477168779264 2016-01-01 06:08:00   \n",
       "2016-01-01 06:07:28      False   682805320217980929 2016-01-01 06:07:00   \n",
       "2016-01-01 05:18:23      False   682792967736848385 2016-01-01 05:18:00   \n",
       "\n",
       "                    tweet_freq  \n",
       "date                            \n",
       "2020-07-11 18:44:28   00:00:31  \n",
       "2020-07-11 18:43:57   00:00:10  \n",
       "2020-07-11 18:43:47   00:00:52  \n",
       "2020-07-11 18:42:55   05:26:55  \n",
       "2020-07-11 13:16:00   00:00:42  \n",
       "...                        ...  \n",
       "2016-01-01 23:00:09   01:30:13  \n",
       "2016-01-01 21:29:56   15:21:50  \n",
       "2016-01-01 06:08:06   00:00:38  \n",
       "2016-01-01 06:07:28   00:49:05  \n",
       "2016-01-01 05:18:23        NaT  \n",
       "\n",
       "[24020 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Time Betweetn Tweets\n",
    "twitter_df['tweet_freq'] = twitter_df['date'].diff(-1)\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:05:48.263234Z",
     "start_time": "2020-07-12T01:05:48.236013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>tweet_minute</th>\n",
       "      <th>tweet_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_minute</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2020-07-11 18:44:00</th>\n",
       "      <th>2020-07-11 18:44:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...</td>\n",
       "      <td>2020-07-11 18:44:28</td>\n",
       "      <td>4728</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022963987054592</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...</td>\n",
       "      <td>2020-07-11 18:43:57</td>\n",
       "      <td>8574</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022835867791361</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...</td>\n",
       "      <td>2020-07-11 18:43:47</td>\n",
       "      <td>6188</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022791857025025</td>\n",
       "      <td>2020-07-11 18:44:00</td>\n",
       "      <td>00:00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 18:43:00</th>\n",
       "      <th>2020-07-11 18:42:55</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...</td>\n",
       "      <td>2020-07-11 18:42:55</td>\n",
       "      <td>3612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1282022576441700356</td>\n",
       "      <td>2020-07-11 18:43:00</td>\n",
       "      <td>05:26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <th>2020-07-11 13:16:00</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>39286</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1281940302782136320</td>\n",
       "      <td>2020-07-11 13:16:00</td>\n",
       "      <td>00:00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     source  \\\n",
       "tweet_minute        date                                      \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28  Twitter for iPhone   \n",
       "                    2020-07-11 18:43:57  Twitter for iPhone   \n",
       "                    2020-07-11 18:43:47  Twitter for iPhone   \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55  Twitter for iPhone   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00  Twitter for iPhone   \n",
       "\n",
       "                                                                                                                                     content  \\\n",
       "tweet_minute        date                                                                                                                       \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28  RT @GreggJarrett: Pelosi Shrugs Off Mobs ‘People Will Do What They Do’ Tells Young People Everyt...   \n",
       "                    2020-07-11 18:43:57  RT @GreggJarrett: Trump right to commute Roger Stone’s sentence – Stone committed no crime was f...   \n",
       "                    2020-07-11 18:43:47  RT @GreggJarrett: Biased anti-Flynn rogue judge exceeds authority by refusing to dismiss wrongfu...   \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55  RT @TVNewsHQ: Fox News’ @GreggJarrett on President Trump’s commutation of Roger Stone sentence: ...   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00      RT @realDonaldTrump: President Trump Approval Rating in the Republican Party at 96%. Thank You!   \n",
       "\n",
       "                                                       date  retweet_count  \\\n",
       "tweet_minute        date                                                     \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28 2020-07-11 18:44:28           4728   \n",
       "                    2020-07-11 18:43:57 2020-07-11 18:43:57           8574   \n",
       "                    2020-07-11 18:43:47 2020-07-11 18:43:47           6188   \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55 2020-07-11 18:42:55           3612   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00 2020-07-11 13:16:00          39286   \n",
       "\n",
       "                                         favorite_count is_retweet  \\\n",
       "tweet_minute        date                                             \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28               0       True   \n",
       "                    2020-07-11 18:43:57               0       True   \n",
       "                    2020-07-11 18:43:47               0       True   \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55               0       True   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00               0       True   \n",
       "\n",
       "                                                      id_str  \\\n",
       "tweet_minute        date                                       \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28  1282022963987054592   \n",
       "                    2020-07-11 18:43:57  1282022835867791361   \n",
       "                    2020-07-11 18:43:47  1282022791857025025   \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55  1282022576441700356   \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00  1281940302782136320   \n",
       "\n",
       "                                               tweet_minute tweet_freq  \n",
       "tweet_minute        date                                                \n",
       "2020-07-11 18:44:00 2020-07-11 18:44:28 2020-07-11 18:44:00   00:00:31  \n",
       "                    2020-07-11 18:43:57 2020-07-11 18:44:00   00:00:10  \n",
       "                    2020-07-11 18:43:47 2020-07-11 18:44:00   00:00:52  \n",
       "2020-07-11 18:43:00 2020-07-11 18:42:55 2020-07-11 18:43:00   05:26:55  \n",
       "2020-07-11 13:16:00 2020-07-11 13:16:00 2020-07-11 13:16:00   00:00:42  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set MultiIndex to Easily View Rapid Tweets\n",
    "twitter_ts = twitter_df.set_index(['tweet_minute','date',],drop=False).copy()\n",
    "twitter_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T01:07:07.795904Z",
     "start_time": "2020-07-12T01:07:07.793843Z"
    }
   },
   "outputs": [],
   "source": [
    "# twitter_ts[twitter_ts.duplicated(keep=False,subset=['tweet_minute'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📕 BOOKMARK 07/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Offsets to Test Different Delta_Price time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T00:57:42.445173Z",
     "start_time": "2020-07-12T00:57:42.442149Z"
    }
   },
   "outputs": [],
   "source": [
    "Min = pd.tseries.offsets.Minute()\n",
    "BH = pd.tseries.offsets.BusinessHour(start='09:30',end='16:30')\n",
    "BD = pd.tseries.offsets.BusinessDay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting Code for Combining Stocks and Tweets as Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:56:49.412317Z",
     "start_time": "2020-02-27T16:56:49.201997Z"
    }
   },
   "outputs": [],
   "source": [
    "## LOAD IN RAW TWITTER DATA, NO PROCESSING\n",
    "# from functions_combined_BEST import ihelp_menu2\n",
    "# # file_dict = ji.load_filename_directory()\n",
    "raw_tweets = 'data/trump_tweets_12012016_to_01012020.csv'\n",
    "sp500 = 'data/SP500_1min_01_23_2020.xlsx'\n",
    "# dft = pd.read_csv(tweets)\n",
    "# dfs = pd.read_excel(sp500)\n",
    "np.random.seed(42)\n",
    "\n",
    "twitter_df= ji.load_raw_twitter_file(filename= raw_tweets,#'data/trump_tweets_12012016_to_01012020.csv'\n",
    "\n",
    "                                     date_as_index=True,\n",
    "                                     rename_map={'text': 'content', 'created_at': 'date_time_index'})\n",
    "twitter_df = ji.check_twitter_df(twitter_df,text_col='content',remove_duplicates=True, remove_long_strings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:56:50.399900Z",
     "start_time": "2020-02-27T16:56:50.265507Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAKE TIME INTERVALS BASED ON BUSINESS HOUR START (09:30-10:30)\n",
    "time_intervals= \\\n",
    "ji.make_time_index_intervals(stock_df.reset_index(drop=True),\n",
    "                             col='date_time_index', \n",
    "                             closed='right',\n",
    "                             return_interval_dicts=False) \n",
    "time_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.568586Z",
     "start_time": "2020-02-27T16:45:27.429Z"
    }
   },
   "outputs": [],
   "source": [
    "## USE THE TIME INDEX TO FILTER OUT TWEETS FROM THE HOUR PRIOR\n",
    "twitter_df, bin_codes = ji.bin_df_by_date_intervals(twitter_df ,time_intervals)\n",
    "stock_df, bin_codes_stock = ji.bin_df_by_date_intervals(stock_df.reset_index(drop=True), time_intervals, column='date_time_index')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.569549Z",
     "start_time": "2020-02-27T16:45:27.434Z"
    }
   },
   "outputs": [],
   "source": [
    "## COLLAPSE DFs BY CODED BINS\n",
    "twitter_grouped = ji.collapse_df_by_group_index_col(twitter_df,\n",
    "                                                    group_index_col='int_bins',\n",
    "                                                    drop_orig=True,\n",
    "                                                    verbose=0)\n",
    "\n",
    "stocks_grouped = ji.collapse_df_by_group_index_col(stock_df,\n",
    "                                                    drop_orig=True,\n",
    "                                                    group_index_col='int_bins', \n",
    "                                                  verbose=0)\n",
    "display(twitter_grouped.head(2),stocks_grouped.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.570390Z",
     "start_time": "2020-02-27T16:45:27.439Z"
    }
   },
   "outputs": [],
   "source": [
    "# 022720\n",
    "stocks_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.571743Z",
     "start_time": "2020-02-27T16:45:27.444Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu(ji.merge_stocks_and_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.572766Z",
     "start_time": "2020-02-27T16:45:27.450Z"
    }
   },
   "outputs": [],
   "source": [
    "## STOCKS AND TWEETS \n",
    "stocks_grouped['date_time'] = stocks_grouped.index.to_series()\n",
    "df_combined = ji.merge_stocks_and_tweets(stocks_grouped, \n",
    "                                      twitter_grouped,\n",
    "                                      on='int_bins',how='left',\n",
    "                                      show_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIGINAL ORDER RESUMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:07.922954Z",
     "start_time": "2020-02-28T02:27:58.931401Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. create minimally cleaned column `content_min_clean` with urls\n",
    "twitter_df = ji.full_twitter_df_processing(twitter_df,\n",
    "                                           raw_tweet_col='content',\n",
    "                                           name_for_cleaned_tweet_col='content_cleaned',\n",
    "                                           name_for_stopped_col='cleaned_stopped_content', \n",
    "                                           name_for_tokenzied_stopped_col='cleaned_stopped_tokens',\n",
    "                                           use_col_for_case_ratio='content', \n",
    "                                           use_col_for_sentiment='content_min_clean',\n",
    "                                           RT=True, urls=True, hashtags=True, mentions=True,\n",
    "                                           str_tags_mentions=True,force=True)\n",
    "#                                            stopwords_list=frequent_words, force=True)\n",
    "#                                            stopwords_list=stop_words, force=True)\n",
    "## Display Index information\n",
    "ji.index_report(twitter_df,label='twitter_df')\n",
    "\n",
    "## Check for strings that exceed the correct tweet length\n",
    "keep_idx = ji.check_length_string_column(twitter_df, 'content_min_clean',length_cutoff=400,display_describe=False)\n",
    "## verify no issues arise.\n",
    "if keep_idx.isna().sum()>0:\n",
    "    raise Exception('')\n",
    "else:\n",
    "    twitter_df=twitter_df[keep_idx]\n",
    "    print(f'removed {np.sum(keep_idx==False)}')\n",
    "\n",
    "ji.check_length_string_column(twitter_df, 'content_min_clean',length_cutoff=400,return_keep_idx=False)\n",
    "twitter_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New - 09/21/19\n",
    "- Removing 'starts_RT' tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:07.944880Z",
     "start_time": "2020-02-28T02:28:07.925726Z"
    }
   },
   "outputs": [],
   "source": [
    "display(twitter_df['is_retweet'].value_counts())\n",
    "\n",
    "twitter_df = twitter_df.query('is_retweet == False')\n",
    "display(twitter_df['is_retweet'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:07.948325Z",
     "start_time": "2020-02-28T02:28:07.946395Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Dropping columns about retweets\n",
    "# drop_cols = ['has_RT','content_starts_RT','starts_RT']\n",
    "# twitter_df.drop(drop_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:07.958232Z",
     "start_time": "2020-02-28T02:28:07.949476Z"
    }
   },
   "outputs": [],
   "source": [
    "from fsds_100719.imports import *\n",
    "fs.ihelp(ji.display_same_tweet_diff_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:08.002623Z",
     "start_time": "2020-02-28T02:28:07.959446Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.display_same_tweet_diff_cols(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:08.038725Z",
     "start_time": "2020-02-28T02:28:08.003920Z"
    }
   },
   "outputs": [],
   "source": [
    "## Search all tweets for occurances of specific words\n",
    "word = 'fed'\n",
    "idx_russia_tweets = ji.search_for_tweets_with_word(twitter_df, word =word,\n",
    "                                     display_n=5, from_column='content',\n",
    "                                     return_index=True, display_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating `delta_price_class` for Each Tweet\n",
    "#### Using S&P 500 Price  1-min-resolution for `delta_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:08.113974Z",
     "start_time": "2020-02-28T02:28:08.039977Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "func_list = [ji.load_raw_stock_data_from_txt, \n",
    "            ji.set_timeindex_freq,\n",
    "            ji.load_twitter_df_stock_price]\n",
    "\n",
    "ji.ihelp_menu(func_list)\n",
    "ji.save_ihelp_menu_to_file(func_list,'_stock_data_to_twitter_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Delta Stock Price Data - For *Each* Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:08.166796Z",
     "start_time": "2020-02-28T02:28:08.115351Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ji.column_report(twitter_df,as_qgrid=False,as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:08.194830Z",
     "start_time": "2020-02-28T02:28:08.168277Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"[i] # number of tweets = {twitter_df.shape[0]}\")\n",
    "\n",
    "## add stock_price for twitter_df\n",
    "null_ratio = ji.check_null_small(twitter_df,null_index_column='case_ratio')\n",
    "\n",
    "print(f'[!] {len(null_ratio)} null values for \"case_ratio\" are tweets containing only urls. Dropping...')\n",
    "twitter_df.dropna(subset=['is_retweet','case_ratio'],inplace=True)\n",
    "print(f\"[i] New # of tweets = {twitter_df.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:43.132838Z",
     "start_time": "2020-02-28T02:28:08.196077Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "# stock_df = pd.read_excel('data/SP500_1min_01_23_2020.xlsx')\n",
    "stock_df = pd.read_excel('data/SP500_1min_01_23_2020.xlsx',parse_dates=False)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:45.283839Z",
     "start_time": "2020-02-28T02:28:43.134524Z"
    }
   },
   "outputs": [],
   "source": [
    "date_time_index = stock_df[\"Date\"].astype('str') + ' '+stock_df['Time'].astype('str')\n",
    "stock_df['date_time_index'] = pd.to_datetime(date_time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:28:45.287067Z",
     "start_time": "2020-02-28T02:28:45.285131Z"
    }
   },
   "outputs": [],
   "source": [
    "# date_time_index = (stock_df['Date'].astype('str')+' '+stock_df['Time'])#.rename('date_time_index')\n",
    "# date_time_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T23:46:46.668031Z",
     "start_time": "2020-01-24T23:45:37.623Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:29:37.990751Z",
     "start_time": "2020-02-28T02:28:45.288713Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df = ji.load_twitter_df_stock_price(twitter_df, \n",
    "                                            stock_price_file='data/SP500_1min_01_23_2020.xlsx',\n",
    "                                           get_stock_prices_per_tweet=True,\n",
    "                                           price_mins_after_tweet=60)\n",
    "\n",
    "ji.index_report(twitter_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:29:38.059311Z",
     "start_time": "2020-02-28T02:29:37.992645Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_null_delta = ji.check_null_small(twitter_df,null_index_column='delta_price');\n",
    "print(f\"[!] {len(idx_null_delta)} null values for 'delta_price' were off-hour tweets,\\\n",
    "more than 1 day before the market reopened. Dropping...\")\n",
    "twitter_df.dropna(subset=['delta_price'], inplace=True)\n",
    "\n",
    "print(f\"\\n[i] Final # of tweets = {twitter_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:29:38.062804Z",
     "start_time": "2020-02-28T02:29:38.060588Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ji.column_report(twitter_df, as_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Cutoffs for Delta Price Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:29:38.078388Z",
     "start_time": "2020-02-28T02:29:38.064110Z"
    }
   },
   "outputs": [],
   "source": [
    "## Examine delta_price\n",
    "print(\"CURRENT # OF POSTITIVE AND NEGATIVE PRICE DELTAS:\")\n",
    "print(twitter_df['delta_price_class'].value_counts())\n",
    "\n",
    "## Examining Changes to classes if use a \"No Change\" cutoff of $0.05\n",
    "delta_price = twitter_df['delta_price']\n",
    "small_pos =[ 0 < x <.05 for x in delta_price] #/len(delta_price)\n",
    "small_neg = [-.05<x <0 for x in delta_price]\n",
    "\n",
    "print('\\nCHANGES TO CLASSES IF USING ATHRESHOLD OF $0.05:\\n','---'*12)\n",
    "print(f'# Positive Delta -> \"No Change\" = {np.sum(small_pos)}')\n",
    "print(f'# Negative Delta -> \"No Change\" = {np.sum(small_neg)}')\n",
    "print(f'# of Unchanged Classifications =  {len(delta_price)-(np.sum(small_pos)+np.sum(small_neg))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:29:49.654895Z",
     "start_time": "2020-02-28T02:29:49.613232Z"
    }
   },
   "outputs": [],
   "source": [
    "## BIN DELTA PRICE CLASS\n",
    "bins = pd.IntervalIndex.from_tuples([ (-np.inf,-.05), (-.05,.05), (.05,np.inf)], closed='left')\n",
    "## Save indexer column for 'delta_price'\n",
    "twitter_df['indexer'] = bins.get_indexer(twitter_df['delta_price'])\n",
    "\n",
    "# remap -1,0,1,2 to classes\n",
    "mapper ={-1:np.nan, 0:0, 1:1,2:2}\n",
    "# remap string classes\n",
    "mapper2 = {0:'neg', 1:'no_change',2:'pos'}\n",
    "\n",
    "## Use indexer to map new integer values\n",
    "twitter_df['delta_price_class_int']= twitter_df['indexer'].apply(lambda x: mapper[x])\n",
    "twitter_df['delta_price_class'] = twitter_df['delta_price_class_int'].apply(lambda x: mapper2[x])\n",
    "\n",
    "## Verify mapping of string and integer classes\n",
    "res1 = pd.DataFrame(twitter_df['delta_price_class'].value_counts())\n",
    "res2 = pd.DataFrame(twitter_df['delta_price_class_int'].value_counts())\n",
    "bs.display_side_by_side(res1,res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE/VISUALIZE \n",
    "### Delta Price Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:30:24.722714Z",
     "start_time": "2020-02-28T02:30:23.282994Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df.to_csv('data/_twitter_df_with_stock_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:00.672036Z",
     "start_time": "2020-02-27T16:46:59.371169Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.plotly_price_histogram(twitter_df,show_fig=True,as_figure=False)\n",
    "ji.plotly_pie_chart(twitter_df, column_to_plot='delta_price_class',show_fig=True, as_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP by Delta Price Class\n",
    "\n",
    "- For comparison of words and bigrams, we will exclude the 'no change' class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:00.689416Z",
     "start_time": "2020-02-27T16:47:00.673297Z"
    }
   },
   "outputs": [],
   "source": [
    "# twitter_df['delta_price_class']\n",
    "bs.check_null_small(twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:00.734725Z",
     "start_time": "2020-02-27T16:47:00.690663Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_df = twitter_df.loc[twitter_df['delta_price_class']!='no_change'].copy()\n",
    "# nlp_df.dropna(inplace=True)\n",
    "nlp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:27:18.153507Z",
     "start_time": "2020-02-27T17:27:18.142082Z"
    }
   },
   "outputs": [],
   "source": [
    "fs.ihelp(ji.compare_word_clouds,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:03.625192Z",
     "start_time": "2020-02-27T16:47:00.735826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate wordclouds\n",
    "twitter_df_groups,twitter_group_text = ji.get_group_texts_for_word_cloud(nlp_df, \n",
    "                                                                      text_column='cleaned_stopped_lemmas', \n",
    "                                                                      groupby_column='delta_price_class')\n",
    "\n",
    "\n",
    "ji.compare_word_clouds(text1=twitter_df_groups['pos']['joined'],\n",
    "                       label1='Stock Market Increased',\n",
    "                       text2= twitter_df_groups['neg']['joined'],\n",
    "                       label2='Stock Market Decreased',\n",
    "                       twitter_shaped = True, verbose=1,\n",
    "                       suptitle_y_loc=0.75,\n",
    "                       suptitle_text='Most Frequent Words by Stock Price +/- Change',\n",
    "                       wordcloud_cfg_dict={'collocations':True},\n",
    "                       save_file=True,filepath_folder='',\n",
    "#                        png_filename=file_dict['nlp_figures']['word_clouds_compare'],\n",
    "                      **{'subplot_titles_fontdict':{'fontsize':26,'fontweight':'bold'},\n",
    "                        'suptitle_fontdict':{'fontsize':40,'fontweight':'bold'},\n",
    "                         'group_colors':{'group1':'green','group2':'red'},\n",
    "                        });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:05.879500Z",
     "start_time": "2020-02-27T16:47:03.627176Z"
    }
   },
   "outputs": [],
   "source": [
    "## Comparing words ONLY unique to each group\n",
    "df_pos_words, df_neg_words = ji.compare_freq_dists_unique_words(text1=twitter_df_groups['pos']['text_tokens'],\n",
    "                                                                label1='Price Increased',\n",
    "                                                                text2=twitter_df_groups['neg']['text_tokens'],\n",
    "                                                                label2='Price Decreased',\n",
    "                                                                top_n=20, display_dfs=True,\n",
    "                                                                return_as_dicts=False)\n",
    "\n",
    "pos_freq_dict, neg_freq_dict = ji.compare_freq_dists_unique_words(text1=twitter_df_groups['pos']['text_tokens'],\n",
    "                                                                label1='Price Increased',\n",
    "                                                                text2=twitter_df_groups['neg']['text_tokens'],\n",
    "                                                                label2='Price Decreased',\n",
    "                                                                top_n=20, display_dfs=False,\n",
    "                                                                return_as_dicts=True)\n",
    "\n",
    "\n",
    "## WORDCLOUD OF WORDS UNIQUE TO TWEETS THAT INCREASED VS DECREASED STOCK PRICE\n",
    "ji.compare_word_clouds(text1= pos_freq_dict,label1='Stock Price Increased',\n",
    "                       text2=neg_freq_dict, label2='Stock Price Decreased',\n",
    "                       twitter_shaped=True, from_freq_dicts=True,\n",
    "                       suptitle_y_loc=0.75,wordcloud_cfg_dict={'collocations':True},\n",
    "                       suptitle_text='Words Unique to Stock Price +/- Change',\n",
    "                       save_file=True,filepath_folder='',\n",
    "#                        png_filename=file_dict['nlp_figures']['word_clouds_compare_unique'],\n",
    "                       **{'subplot_titles_fontdict':\n",
    "                         {'fontsize':26,\n",
    "                         'fontweight':'bold'},\n",
    "                        'suptitle_fontdict':{\n",
    "                         'fontsize':40,\n",
    "                         'fontweight':'bold'},\n",
    "                         'group_colors':{\n",
    "                             'group1':'green','group2':'red'}\n",
    "                        });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:06.491991Z",
     "start_time": "2020-02-27T16:47:05.881161Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.make_tweet_bigrams_by_group(twitter_df_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEET DELTA PRICE CLASSIFICATON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embeddings with Word2Vec\n",
    "\n",
    "- Good Parameters for Non-Retweets:\n",
    "```python\n",
    "params = {\n",
    "'text_column': 'cleaned_stopped_lemmas',\n",
    "'window':3,\n",
    "'min_count':2,\n",
    "'epochs':10,\n",
    "'sg':1, \n",
    "'hs':0,\n",
    "'negative':5,\n",
    "'ns_exponent':-0.5\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:06.557662Z",
     "start_time": "2020-02-27T16:47:06.493465Z"
    }
   },
   "outputs": [],
   "source": [
    "func_list = [ji.make_word2vec_model,ji.get_wv_from_word2vec]#,\n",
    "            #ji.get_w2v_kwargs,ji.Word2vecParams]\n",
    "ihelp_menu(func_list)\n",
    "ji.save_ihelp_menu_to_file(func_list,'_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:06.564825Z",
     "start_time": "2020-02-27T16:47:06.558941Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading custom class for tracking Word2Vec parameters\n",
    "w2vParams = ji.Word2vecParams()        \n",
    "w2vParams.params_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:06.573051Z",
     "start_time": "2020-02-27T16:47:06.565915Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.088222Z",
     "start_time": "2020-02-27T16:47:06.574221Z"
    }
   },
   "outputs": [],
   "source": [
    "## FITING WORD2VEC AND TOKENIZER    \n",
    "model_kwds = {\n",
    "# 'text_column': 'cleaned_stopped_lemmas',\n",
    "'window':3,\n",
    "'min_count':2,\n",
    "'epochs':10,\n",
    "'sg':1, \n",
    "'hs':0,\n",
    "'negative':5,\n",
    "'ns_exponent':-0.5\n",
    "}\n",
    "# model_kwds=  ji.get_w2v_kwargs(params)    \n",
    "\n",
    "# text_data = twitter_df[params['text_column']]\n",
    "## using df_tokenize for full body of a text for word2vec\n",
    "word2vec_model = ji.make_word2vec_model(twitter_df,\n",
    "                                        text_column = 'cleaned_stopped_lemmas',\n",
    "                                        verbose=1,\n",
    "                                        return_full=True,\n",
    "                                        **model_kwds)\n",
    "\n",
    "# w2vParams.append(params)\n",
    "\n",
    "wv = word2vec_model.wv\n",
    "\n",
    "### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODEL\n",
    "wv = word2vec_model.wv\n",
    "def V(string,wv=wv):\n",
    "    return wv.get_vector(string)\n",
    "def equals(vector,wv=wv):\n",
    "    return wv.similar_by_vector(vector)\n",
    "\n",
    "list_of_equations = [\"V('republican')-V('honor')\",\n",
    "                    \"V('man')+V('power')\",\n",
    "                     \"V('russia')+V('honor')\",\n",
    "                     \"V('china')+V('tariff')\",\n",
    "                     \"V('flip')+V('lie')\"]\n",
    "\n",
    "for eqn in list_of_equations:\n",
    "    print(f'\\n* {eqn} =')\n",
    "    res = eval(f\"equals({eqn})\")\n",
    "    [print('\\t',x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.091452Z",
     "start_time": "2020-02-27T16:47:10.089508Z"
    }
   },
   "outputs": [],
   "source": [
    "# import functions_io as io\n",
    "# io.save_word2vec(word2vec_model,file_dict,parms_dict=w2vParams.last_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained GloVe Twitter Vectors \n",
    "- 02-27-20 Moved to modeling section due to X_train requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Word Math W2V vs Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-Trained GloVe Twitter download http://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.095508Z",
     "start_time": "2020-02-27T16:47:10.092946Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save Glove Twitter Vectors to A Glove File For Word Math\n",
    "folder_for_vectors ='/Users/jamesirving/Datasets/glove.twitter.27B/'\n",
    "glove_file = 'glove.twitter.27B.100d.txt'\n",
    "\n",
    "# # define full input glove and output word2vec filepaths\n",
    "# glove_filepath = folder_for_vectors+glove_file\n",
    "# w2v_filepath = folder_for_vectors+'glove_to_w2vec.txt'\n",
    "# print(w2v_filepath)\n",
    "# print(glove_filepath)\n",
    "\n",
    "# ## Load in saved glove vectors using KeyedVectors.load_word2vec_format()\n",
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_filepath,w2v_filepath)\n",
    "# glovew2v_model = KeyedVectors.load_word2vec_format(w2v_filepath)\n",
    "# wvg = glovew2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.099363Z",
     "start_time": "2020-02-27T16:47:10.096860Z"
    }
   },
   "outputs": [],
   "source": [
    "# def V(string,wv=wvg):\n",
    "#     return wv.get_vector(string)\n",
    "\n",
    "# def equals(vector,wv=wvg):\n",
    "#     return wv.similar_by_vector(vector)\n",
    "\n",
    "# list_of_equations = [\"V('republican')-V('honor')\",\n",
    "#                     \"V('man')+V('power')\",\n",
    "#                      \"V('russia')+V('honor')\",\n",
    "#                      \"V('china')+V('tariff')\",\n",
    "#                      \"V('trump')+V('lie')\"]\n",
    "\n",
    "# for eqn in list_of_equations:\n",
    "#     print(f'\\n* {eqn} =')\n",
    "#     res = eval(f\"equals({eqn})\")\n",
    "#     [print('\\t',x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.115765Z",
     "start_time": "2020-02-27T16:47:10.100717Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select smaller subset of twitter_df for df_tokenize\n",
    "columns_for_model_0 = ['delta_price_class','delta_price','pre_tweet_price',\n",
    "                       'post_tweet_price','delta_time','B_ts_rounded','B_ts_post_tweet','content',\n",
    "                       'content_min_clean','cleaned_stopped_content','cleaned_stopped_tokens',\n",
    "                       'cleaned_stopped_lemmas','delta_price_class_int']\n",
    "\n",
    "df_tokenize=twitter_df[columns_for_model_0].copy()\n",
    "ji.check_class_balance(df_tokenize,'delta_price_class_int',as_raw=True, as_percent=False)\n",
    "ji.check_class_balance(df_tokenize,'delta_price_class',as_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.179169Z",
     "start_time": "2020-02-27T16:47:10.117154Z"
    }
   },
   "outputs": [],
   "source": [
    "# ji.save_ihelp_to_file(ji.undersample_df_to_match_classes)\n",
    "ihelp_menu([ji.undersample_df_to_match_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.273413Z",
     "start_time": "2020-02-27T16:47:10.180679Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## RESTRICTING TIME DELTAS FOR MODEL\n",
    "remove_delta_time_tweets=True\n",
    "\n",
    "## RESAMPLING \n",
    "undersample_to_match_classes = True#True\n",
    "class_column='delta_price_class'\n",
    "class_list_to_keep = None # None=all classes or ['neg','pos']\n",
    "\n",
    "## Display results\n",
    "show_tweet_versions = True\n",
    "\n",
    "\n",
    "print('[0] INITIAL CLASS COUNTS.')\n",
    "## Print initial class balance\n",
    "ji.check_class_balance(df_tokenize,col=class_column);\n",
    "\n",
    "## REMOVE TWEETS BASED ON TIME BETWEEN TWEET AND STOCK PRICE VALUE\n",
    "if remove_delta_time_tweets:\n",
    "    ## SAMPLE ONLY TWEETS WITHIN 1 DAY OF STOCK MARKET PRICE DATA\n",
    "    df_sampled = df_tokenize.loc[df_tokenize['delta_time']<'1 day']\n",
    "    print(f\"[1] # OF DAYS REMOVED BY 'delta_time' = {df_tokenize.shape[0]-df_sampled.shape[0]}\")\n",
    "    ji.check_class_balance(df_sampled, col=class_column, as_raw=True, as_percent=False)\n",
    "else:\n",
    "    print('[1] Skipping removing tweets by time_delta')\n",
    "    df_sampled = df_tokenize\n",
    "    \n",
    "    \n",
    "## UNDERSAMPLE FROM UNBALANCED CLASSES\n",
    "if undersample_to_match_classes:\n",
    "    \n",
    "    ## Print status\n",
    "    if class_list_to_keep is None:\n",
    "        print_class_list= list(df_sampled[class_column].unique())\n",
    "    else:\n",
    "        print_class_list = class_list_to_keep\n",
    "    print(f'[2] RESAMPLING DF TO MATCH SMALLEST CLASS.\\n\\tBalancing: {print_class_list}')\n",
    "    \n",
    "    ## RESAMPLE TO MATCH CLASSES\n",
    "    df_sampled = ji.undersample_df_to_match_classes(df_sampled,\n",
    "                                                    class_column=class_column,\n",
    "                                                    class_values_to_keep=class_list_to_keep,verbose=0)\n",
    "    ji.check_class_balance(df_sampled,col=class_column, as_percent=False)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print('\\n[2] Skipping balancing classes and keeping all 3 classes.')\n",
    "\n",
    "## Display final output\n",
    "dash = '---'*20\n",
    "print(f\"\\n\\n [i] Final class balance:\")\n",
    "ji.check_class_balance(df_sampled,col=class_column)\n",
    "\n",
    "display(df_sampled.head(2))\n",
    "\n",
    "show_tweet_versions=True\n",
    "if show_tweet_versions:\n",
    "    ji.display_same_tweet_diff_cols(df_sampled,\n",
    "                                    columns = ['content' ,'content_min_clean',\n",
    "                                               'cleaned_stopped_content',\n",
    "                                               'cleaned_stopped_tokens',\n",
    "                                              'cleaned_stopped_lemmas'],as_md=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.279744Z",
     "start_time": "2020-02-27T16:47:10.274643Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_class_balance(df_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, X,y train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.282697Z",
     "start_time": "2020-02-27T16:47:10.280988Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## \n",
    "# wv = ji.get_wv_from_word2vec(word2vec_model)\n",
    "# print(f'Word2Vec Model:\\n\\tThere are {len(wv.vocab)} words with vector size {wv.vector_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:10.285706Z",
     "start_time": "2020-02-27T16:47:10.283941Z"
    }
   },
   "outputs": [],
   "source": [
    "# fname = 'shared_memory/df_sampled_test_nlp_models.csv'\n",
    "# df_sampled.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:12.479236Z",
     "start_time": "2020-02-27T16:47:10.286936Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "text_data = df_sampled['cleaned_stopped_lemmas']\n",
    "\n",
    "# Changed for class imblanace  #\n",
    "y = to_categorical(df_sampled['delta_price_class_int'],num_classes=3)\n",
    "print(f'y.shape={y.shape}')\n",
    "tokenizer = Tokenizer(num_words=len(wv.vocab))\n",
    "\n",
    "## FIGURE OUT WHICH VERSION TO USE WITH SERIES:\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "# return integer-encoded sentences\n",
    "X = tokenizer.texts_to_sequences(text_data)\n",
    "X = sequence.pad_sequences(X)\n",
    "MAX_SEQUENCE_LENGTH = X.shape[1]\n",
    "print(f'Sequence length: {MAX_SEQUENCE_LENGTH}')\n",
    "\n",
    "## Save word indices\n",
    "word_index = tokenizer.index_word\n",
    "reverse_index = {v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:12.487339Z",
     "start_time": "2020-02-27T16:47:12.480709Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get training/test split\n",
    "X_train, X_test,X_val, y_train, y_test,y_val = ji.train_test_val_split(X, y, test_size=0.15, val_size=0.15)\n",
    "\n",
    "# ji.check_y_class_balance(data=[y_train,y_test])\n",
    "print('Training Data:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Test Data:')\n",
    "print(X_test.shape, y_test.shape)\n",
    "print('Val Data:')\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:34:43.189962Z",
     "start_time": "2020-02-27T17:34:43.183817Z"
    }
   },
   "outputs": [],
   "source": [
    "fs.ihelp(ji.load_glove_embeddings,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:59.778345Z",
     "start_time": "2020-02-27T16:47:12.488586Z"
    }
   },
   "outputs": [],
   "source": [
    "## ADDING PRETRAINED GLOVE EMBEDDINGS\n",
    "fp = r'/Users/jamesirving/Datasets/glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "word2index, embedding_matrix_gl = ji.load_glove_embeddings(fp,\n",
    "                                                           encoding='utf-8',\n",
    "                                                           embedding_dim=100,\n",
    "                                                           X_train=X_train,\n",
    "                                                          as_layer=False)\n",
    "\n",
    "vocab_size = embedding_matrix_gl.shape[0]\n",
    "vector_size = embedding_matrix_gl.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:59.782997Z",
     "start_time": "2020-02-27T16:47:59.779806Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_best_nlp_model(model):\n",
    "    model_key='nlp_model_for_predictions'\n",
    "    filename = file_dict[model_key]['base_filename']\n",
    "    nlp_files = ji.save_model_weights_params(model0,check_if_exists=True,auto_increment_name=True, \n",
    "                                         auto_filename_suffix=True,filename_prefix=filename)\n",
    "    return nlp_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:47:59.850261Z",
     "start_time": "2020-02-27T16:47:59.784874Z"
    }
   },
   "outputs": [],
   "source": [
    "func_list = [ji.make_keras_embedding_layer,ji.make_embedding_matrix,\n",
    "            ji.get_wv_from_word2vec]\n",
    "ihelp_menu(func_list)\n",
    "ji.save_ihelp_to_file(func_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKE KERAS EMBEDDING LAYERS FOR GLOVE AND WORD2VEV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:00.765332Z",
     "start_time": "2020-02-27T16:47:59.851539Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.inspect_variables(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T17:36:48.606992Z",
     "start_time": "2020-02-27T17:36:48.601213Z"
    }
   },
   "outputs": [],
   "source": [
    "fs.ihelp(ji.make_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:00.778803Z",
     "start_time": "2020-02-27T16:48:00.767074Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make Word2Vec embedding matrix for keras\n",
    "embedding_matrix_wv = ji.make_embedding_matrix(word2vec_model)\n",
    "print(f'wv:\\n\\tshape={embedding_matrix_wv.shape}')\n",
    "\n",
    "\n",
    "## MAKE WORD2VEC EMBEDDING LAYER\n",
    "from keras import layers         \n",
    "## Make word2vec embedding layer\n",
    "vocab_size = embedding_matrix_wv.shape[0]#len(wv.vocab)\n",
    "vector_size = embedding_matrix_wv.shape[1]#wv.vector_size\n",
    "\n",
    "embedding_layer_wv =layers.Embedding(vocab_size,#+1\n",
    "                                  vector_size,\n",
    "                                  input_length=X_train.shape[1],\n",
    "                                  weights=[embedding_matrix_wv],\n",
    "                                  trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:01.425003Z",
     "start_time": "2020-02-27T16:48:00.780071Z"
    }
   },
   "outputs": [],
   "source": [
    "## KERAS LSTM WITH WORD2VEC EMBEDDING\n",
    "from keras import callbacks, models, layers, optimizers, regularizers\n",
    "## Make model infrastructure:\n",
    "model0_wv = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "model0_wv.add(embedding_layer_wv)\n",
    "# model0_wv.add(layers.SpatialDropout1D(0.2))\n",
    "\n",
    "# model0_wv.add(layers.Bidirectional(layers.LSTM(units=100, return_sequences=True,\n",
    "#                        dropout=0.5,recurrent_dropout=0.2,\n",
    "#                        kernel_regularizer=regularizers.l2(.01))))\n",
    "model0_wv.add(layers.Bidirectional(layers.LSTM(units=100, return_sequences=False,\n",
    "                       dropout=0.3,recurrent_dropout=0.2,\n",
    "                       kernel_regularizer=regularizers.l2(.01))))\n",
    "\n",
    "model0_wv.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model0_wv.compile(loss='categorical_crossentropy',optimizer=\"rmsprop\",metrics=['acc']) #optimizer=\"adam\"\n",
    "model0_wv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:01.436667Z",
     "start_time": "2020-02-27T16:48:01.426367Z"
    }
   },
   "outputs": [],
   "source": [
    "reload([ji,bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:27.934021Z",
     "start_time": "2020-02-27T16:48:01.437980Z"
    }
   },
   "outputs": [],
   "source": [
    "## set params\n",
    "num_epochs = 10\n",
    "# validation_split = 0.2\n",
    "\n",
    "\n",
    "clock = bs.Clock()\n",
    "clock.tic()\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "history0_wv = model0_wv.fit(X_train, y_train, \n",
    "                      epochs=num_epochs,\n",
    "                      verbose=True, \n",
    "                      validation_data=(X_val,y_val),#validation_split=validation_split,\n",
    "                      batch_size=100)#,\n",
    "#                       callbacks=callbacks)\n",
    "\n",
    "clock.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:31.648804Z",
     "start_time": "2020-02-27T16:48:27.935302Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_fname = 'models/model_0A/fig_conf_mat.ext'\n",
    "hist_fname = 'models/model_0A/fig_keras_history.ext'\n",
    "summary_fname ='models/model_0A/model_summary'\n",
    "\n",
    "df_class_report0A_wv, fig0A_wv=ji.evaluate_classification_model(model0_wv,\n",
    "                                                   X_train, X_test,\n",
    "                                                   y_train, y_test, \n",
    "                                                                history=history0_wv,\n",
    "#                                                    report_as_df=False,\n",
    "                                                   binary_classes=False,\n",
    "                                                   conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                                                   normalize_conf_matrix=True, \n",
    "                                                   save_history=True, history_filename=hist_fname,\n",
    "                                                   save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "                                                   save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:18.219424Z",
     "start_time": "2020-02-27T16:48:31.650204Z"
    }
   },
   "outputs": [],
   "source": [
    "## MAKE GloVe EMBEDDING LAYER\n",
    "# word2index, embedding_matrix_gl = ji.load_glove_embeddings(fp,encoding='utf-8',embedding_dim=100)\n",
    "\n",
    "def load_glove_embedding_layer(fp=None, trainable=False):\n",
    "    if fp is None:\n",
    "        fp = r'/Users/jamesirving/Datasets/glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "\n",
    "    ## Make GloVe embedding matrix\n",
    "    word2index, embedding_matrix_gl = ji.load_glove_embeddings(fp=fp,encoding='utf-8',\n",
    "                                                               embedding_dim=100,\n",
    "                                                              X_train=X_train,\n",
    "                                                              as_layer=False)\n",
    "    print(f'gl:\\n\\tshape={embedding_matrix_gl.shape}')\n",
    "\n",
    "    from keras import layers         \n",
    "    vocab_size = embedding_matrix_gl.shape[0]#len(wv.vocab)\n",
    "    vector_size = embedding_matrix_gl.shape[1]#wv.vector_size\n",
    "\n",
    "    embedding_layer_gl =layers.Embedding(vocab_size,#+1,\n",
    "                                      vector_size,\n",
    "                                      input_length=X_train.shape[1],\n",
    "                                      weights=[embedding_matrix_gl],\n",
    "                                      trainable=trainable)#,X_train=X_train)\n",
    "    return word2index, embedding_layer_gl\n",
    "\n",
    "# glove_word2index, embedding_layer_gl = load_glove_embedding_layer(trainable=True)\n",
    "glove_word2index, embedding_layer_gl = load_glove_embedding_layer(trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:49.054603Z",
     "start_time": "2020-02-27T16:49:18.220941Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import callbacks, models, layers, optimizers, regularizers\n",
    "## Make model infrastructure:\n",
    "model0_gl = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "model0_gl.add(embedding_layer_gl)\n",
    "model0_gl.add(layers.SpatialDropout1D(0.2))\n",
    "\n",
    "# model0_gl.add(layers.Bidirectional(layers.LSTM(units=100, return_sequences=True,\n",
    "#                        dropout=0.5,recurrent_dropout=0.2,\n",
    "#                        kernel_regularizer=regularizers.l2(.01))))\n",
    "model0_gl.add(layers.Bidirectional(layers.LSTM(units=100, return_sequences=False)))#,\n",
    "#                        dropout=0.5,recurrent_dropout=0.2)))#,\n",
    "#                        kernel_regularizer=regularizers.l2(.01))))\n",
    "\n",
    "model0_gl.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model0_gl.compile(loss='categorical_crossentropy',optimizer=\"adadelta\",metrics=['acc']) #optimizer=\"\"rmsprop\"\"\n",
    "print(model0_gl.summary())\n",
    "\n",
    "## set params\n",
    "num_epochs = 15\n",
    "# validation_split = 0.2\n",
    "clock = bs.Clock()\n",
    "clock.tic()\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "history0_gl = model0_gl.fit(X_train, y_train, \n",
    "                      epochs=num_epochs,\n",
    "                      verbose=True, \n",
    "                      validation_data=(X_val,y_val),#validation_split=validation_split,\n",
    "                      batch_size=100)\n",
    "                      \n",
    "\n",
    "clock.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.350287Z",
     "start_time": "2020-02-27T16:49:49.055921Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cm_fname = file_dict['model_0A']['fig_conf_mat.ext']\n",
    "# hist_fname = file_dict['model_0A']['fig_keras_history.ext']\n",
    "# summary_fname = file_dict['model_0A']['model_summary']\n",
    "\n",
    "df_class_report0A_gl,fig0A_gl=ji.evaluate_classification_model(model0_gl,\n",
    "                                                   X_train, X_test,\n",
    "                                                   y_train, y_test, \n",
    "                                                               history=history0_gl,\n",
    "#                                                    report_as_df=False,\n",
    "                                                   binary_classes=False,\n",
    "                                                   conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                                                   normalize_conf_matrix=True)#, \n",
    "#                                                    save_history=True, history_filename=hist_fname,\n",
    "#                                                    save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "#                                                    save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.491361Z",
     "start_time": "2020-02-27T16:49:51.351595Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_model0_gl = model0_gl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.497061Z",
     "start_time": "2020-02-27T16:49:51.492968Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(np.array([0.47, 0.42, 0.18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.510726Z",
     "start_time": "2020-02-27T16:49:51.504358Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_custom_scorer(y_true,y_pred, model=None, **kwargs):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "    from sklearn.metrics import make_scorer,confusion_matrix\n",
    "    \n",
    "    # set labels if provided\n",
    "    if 'labels' in kwargs:\n",
    "        labels = kwargs['labels']\n",
    "    else:\n",
    "        labels=np.unique(y_true)\n",
    "    \n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "    \n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "            \n",
    "    # Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    score = np.mean(diag)\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "    \n",
    "    \n",
    "    ji.plot_confusion_matrix(cm,normalize=True)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.729406Z",
     "start_time": "2020-02-27T16:49:51.513914Z"
    }
   },
   "outputs": [],
   "source": [
    "my_custom_scorer(y_test, y_pred_model0_gl)\n",
    "# np.mean[[0.47058824, 0.42307692 0.18253968]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.733994Z",
     "start_time": "2020-02-27T16:49:51.730755Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_best_nlp_model(model):\n",
    "    model_key='nlp_model_for_predictions'\n",
    "    filename = file_dict[model_key]['base_filename']\n",
    "    nlp_files = ji.save_model_weights_params(model0,check_if_exists=True,auto_increment_name=True, \n",
    "                                         auto_filename_suffix=True,filename_prefix=filename)\n",
    "    return nlp_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:51.737718Z",
     "start_time": "2020-02-27T16:49:51.735513Z"
    }
   },
   "outputs": [],
   "source": [
    "# save_me_as_model_0A=True\n",
    "# save_me_as_pred_nlp = False\n",
    "\n",
    "# ji.reload(ji)\n",
    "# if save_me_as_pred_nlp:\n",
    "#     model_key='nlp_model_for_predictions'\n",
    "\n",
    "# elif save_me_as_model_0A:\n",
    "#     model_key='model_0A'    \n",
    "    \n",
    "# # filename = file_dict[model_key]['base_filename']\n",
    "# nlp_files = ji.save_model_weights_params(model0_gl,check_if_exists=True,auto_increment_name=True, \n",
    "#                                          auto_filename_suffix=True,filename_prefix=filename)\n",
    "\n",
    "# file_dict[model_key]['output_filenames'] = nlp_files\n",
    "\n",
    "# ji.update_file_directory(file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kera's Example Pretrained Word Embeddings Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Parameters \n",
    "https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "\n",
    "- See notebook `keras_hyperparameter_tuning.ipynb`for gridsearch\n",
    "\n",
    "```python\n",
    "# Create HyperParaemeter Space\n",
    "params_to_search ={'filter_size':[3,4,5,6],\n",
    "                   'activation':['relu','tanh','linear'],\n",
    "                   'n_filters':[100,200],#,300,400],\n",
    "                  'dropout':[0.2],\n",
    "                  'optimizer':['adam','rmsprop','adadelta'],\n",
    "                'epochs':[10]}\n",
    "\n",
    "\n",
    "create_model(embedding_layer=embedding_layer_gl,trainable=False,\n",
    "              n_filters=100, filter_size=4,\n",
    "              activation='tanh', optimizer='adadelta',\n",
    "              dropout=0.2,l2_lr=0.01,\n",
    "              batch_size=100,\n",
    "              epochs=10,verbose = 0,show_summary=False)\n",
    "best results={'activation': 'tanh', \n",
    "              'dropout': 0.2,\n",
    "              'epochs': 10,\n",
    "              'filter_size': 4,\n",
    "              'n_filters': 100,\n",
    "              'optimizer': 'adadelta'}\n",
    "```\n",
    "\n",
    "##### Things to Try:\n",
    "1. Weight Regularization (L2):3 \n",
    "2. Adadelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:50:38.460102Z",
     "start_time": "2020-02-27T16:49:51.739279Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get a trainable embedding_layer with GloVe Emebddings\n",
    "_,embedding_layer_gl=load_glove_embedding_layer(trainable=False)\n",
    "\n",
    "\n",
    "## Save the initial weights before any training\n",
    "gl_initial_weights = embedding_layer_gl.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:50:38.474622Z",
     "start_time": "2020-02-27T16:50:38.461414Z"
    }
   },
   "outputs": [],
   "source": [
    "params_to_search ={'filter_size':[3,4,5,6],\n",
    "                   'activation':['relu','tanh','linear'],\n",
    "                   'n_filters':[100,200],#,300,400],\n",
    "                  'dropout':[0.2],\n",
    "                  'optimizer':['adam','rmsprop','adadelta'],\n",
    "                'epochs':[10]}\n",
    "\n",
    "def model_cnn(embedding_layer, X_train, X_test,\n",
    "              y_train,y_test,\n",
    "              X_val, y_val,\n",
    "              n_filters=128, filter_size=4,\n",
    "              activation='relu', optimizer='rmsprop',\n",
    "              dropout=0.2,l2_lr=0.01,\n",
    "              batch_size=128,\n",
    "              epochs=10,verbose = 1,trainable_embedding=False,show_summary=False):\n",
    "    \n",
    "    from keras.layers import Input, Conv1D,GlobalMaxPooling1D, MaxPooling1D, Dense, Dropout\n",
    "    from keras.models import Model\n",
    "    from keras.regularizers import l2\n",
    "    embedding_layer.trainable=trainable_embedding\n",
    "    print(f'Embedding layer trainable={embedding_layer.trainable}')\n",
    "\n",
    "    print('Training model.')\n",
    "    MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer_gl(sequence_input)\n",
    "    ## Specify layer parameters\n",
    "\n",
    "\n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation, data_format='channels_first',\n",
    "               kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(embedded_sequences)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "              kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "              kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    ##\n",
    "    x = Dense(n_filters, activation='relu')(x) #128\n",
    "    preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, #'rmsprop',#adam\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    if show_summary:\n",
    "        display(model.summary())\n",
    "    \n",
    "    \n",
    "    \n",
    "    clock = bs.Clock()\n",
    "    clock.tic(f'units={n_filters}, filter={filter_size},dropout={dropout},opt={optimizer}')\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,verbose=verbose,\n",
    "                        validation_data=(X_val,y_val))\n",
    "\n",
    "              #validation_split=0.1)\n",
    "    clock.toc('')\n",
    "    df_class_report0_CNN,fig0_CNN=ji.evaluate_classification_model(model,\n",
    "                                                       X_train, X_test,\n",
    "                                                       y_train, y_test, \n",
    "#                                                        report_as_df=False,\n",
    "                                                                   history=history,\n",
    "                                                       binary_classes=False,\n",
    "                                                       conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                                                       normalize_conf_matrix=True)\n",
    "    \n",
    "    return model, df_class_report0_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:01.465700Z",
     "start_time": "2020-02-27T16:50:38.476263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Use results from grid search\n",
    "best_results={'activation': 'tanh', \n",
    "              'dropout': 0.2,\n",
    "              'epochs': 10,\n",
    "              'filter_size': 4,\n",
    "              'n_filters': 100,\n",
    "              'optimizer': 'adadelta'}\n",
    "outputs= model_cnn(embedding_layer_gl,\n",
    "                   X_train, X_test,\n",
    "                   y_train, y_test, \n",
    "                   X_val,y_val,l2_lr=0.005,\n",
    "                   trainable_embedding=False,\n",
    "                   \n",
    "                   **best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:01.475335Z",
     "start_time": "2020-02-27T16:51:01.466942Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_model(embedding_layer=embedding_layer_gl,trainable=False,\n",
    "              n_filters=128, filter_size=4,\n",
    "              activation='relu', optimizer='rmsprop',\n",
    "              dropout=0.2,l2_lr=0.01,\n",
    "              batch_size=100,\n",
    "              epochs=10,verbose = 0,show_summary=False):\n",
    "    \n",
    "    embedding_layer.trainable=trainable\n",
    "    print(f'Embedding layer trainable={embedding_layer.trainable}')\n",
    "    from keras.layers import Input, Conv1D,GlobalMaxPooling1D, MaxPooling1D, Dense, Dropout\n",
    "    from keras.models import Model\n",
    "    from keras.regularizers import l2\n",
    "\n",
    "    print('Training model.')\n",
    "    MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    ## Specify layer parameters\n",
    "\n",
    "\n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation, data_format='channels_first',\n",
    "               kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(embedded_sequences)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "              kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "              kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    ##\n",
    "    x = Dense(n_filters, activation=activation )(x) #128'relu'\n",
    "    preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, #'rmsprop',#adam\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    if show_summary:\n",
    "        display(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:01.480466Z",
     "start_time": "2020-02-27T16:51:01.476635Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,y_train,batch_size,epochs,verbose,validation_data):\n",
    "    \n",
    "    clock = bs.Clock()\n",
    "#     clock.tic(f'units={n_filters}, filter={filter_size},dropout={dropout},opt={optimizer}')\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,verbose=verbose,\n",
    "                        validation_data=(X_val,y_val))\n",
    "\n",
    "              #validation_split=0.1)\n",
    "    clock.toc('')\n",
    "    df_class_report0_CNN,fig0_CNN=ji.evaluate_classification(model,history,\n",
    "                                                       X_train, X_test,\n",
    "                                                       y_train, y_test, \n",
    "                                                       report_as_df=False,\n",
    "                                                       binary_classes=False,\n",
    "                                                       conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                                                       normalize_conf_matrix=True)\n",
    "    \n",
    "    return model, df_class_report0_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:01.612358Z",
     "start_time": "2020-02-27T16:51:01.481661Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_model0_gl = model0_gl.predict(X_test)\n",
    "# score, cm_norm = my_custom_scorer(y_test,y_pred_model0_gl)\n",
    "# print(cm_norm, cm_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:01.620325Z",
     "start_time": "2020-02-27T16:51:01.613581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN_GRID_SEARCH=False\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tune_clock = bs.Clock()\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "neural_network = KerasClassifier(build_fn=create_model,verbose=2)    \n",
    "\n",
    "best_results={'activation': ['tanh'], \n",
    "              'dropout': [0.2],\n",
    "              'epochs': [10,15],#,20]\n",
    "              'filter_size': [4,7],\n",
    "              'batch_size':[50,100,300],\n",
    "              'n_filters': [100,300],\n",
    "              'optimizer': ['adadelta'],\n",
    "             'l2_lr':[0.005,0.003,0.03]}\n",
    "grid = GridSearchCV(estimator=neural_network,param_grid=best_results,scoring=make_scorer(my_custom_scorer))\n",
    "\n",
    "\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    tune_clock.tic()\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    tune_clock.toc()\n",
    "    grid.best_params_\n",
    "else:\n",
    "    print('[!] Grid search skipped (RUN_GRID_SEARCH=FALSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:15:22.880538Z",
     "start_time": "2020-02-27T05:07:02.935Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- REsults from overnight gridsearch\n",
    "-params ran:\n",
    "```python\n",
    "params={'activation': ['tanh'], \n",
    "              'dropout': [0.2],\n",
    "              'epochs': [10,15],#,20]\n",
    "              'filter_size': [4,7],\n",
    "              'batch_size':[50,100,300],\n",
    "              'n_filters': [100,300],\n",
    "              'optimizer': ['adadelta'],\n",
    "             'l2_lr':[0.005,0.003,0.03]}\n",
    "```\n",
    "- best _params\n",
    "\n",
    "```python\n",
    "\n",
    "{'activation': 'tanh',\n",
    " 'batch_size': 100,\n",
    " 'dropout': 0.2,\n",
    " 'epochs': 10,\n",
    " 'filter_size': 4,\n",
    " 'l2_lr': 0.005,\n",
    " 'n_filters': 100,\n",
    " 'optimizer': 'adadelta'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.307254Z",
     "start_time": "2020-02-27T16:51:01.621627Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Use results from grid search\n",
    "best_results={'activation': 'tanh', \n",
    "              'dropout': 0.2,\n",
    "              'epochs': 10,\n",
    "              'filter_size': 4,\n",
    "              'n_filters': 100,\n",
    "              'l2_lr':0.005,\n",
    "              'optimizer': 'adadelta'}\n",
    "outputs= model_cnn(embedding_layer_gl,\n",
    "                   X_train, X_test,\n",
    "                   y_train, y_test, \n",
    "                   X_val,y_val,\n",
    "                   trainable_embedding=False,\n",
    "                   **best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.310362Z",
     "start_time": "2020-02-27T16:51:23.308383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.314380Z",
     "start_time": "2020-02-27T16:51:23.311592Z"
    }
   },
   "outputs": [],
   "source": [
    "print(pd.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOKMARK 09/28/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.325667Z",
     "start_time": "2020-02-27T16:51:23.315783Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_deep_model(embedding_layer=embedding_layer_gl,trainable=False,\n",
    "                       n_conv_layers=3,n_filters=128, filter_size=4,\n",
    "                      activation='tanh', optimizer='adadelta',\n",
    "                      dropout=0.2,l2_lr=0.005,\n",
    "                      batch_size=100,\n",
    "                      epochs=10,verbose = 1):\n",
    "    \n",
    "    ## Print out parameters used to create model\n",
    "    vars = locals()\n",
    "    dashes = '---'*20\n",
    "    print('\\n\\n')\n",
    "    print(dashes)\n",
    "    now = pd.datetime.now()\n",
    "    print(now.strftime('%m/%d/%Y-%T'))\n",
    "    print(vars)    \n",
    "    \n",
    "    embedding_layer.trainable=trainable\n",
    "    print(f'Embedding layer trainable={embedding_layer.trainable}')\n",
    "    from keras.layers import Input, Conv1D,GlobalMaxPooling1D, MaxPooling1D, Dense, Dropout\n",
    "    from keras.models import Model\n",
    "    from keras.regularizers import l2\n",
    "\n",
    "    print('Training model.')\n",
    "    MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    ## Specify layer parameters\n",
    "\n",
    "\n",
    "    ##\n",
    "    x = Conv1D(n_filters, filter_size, activation=activation, #data_format='channels_first',\n",
    "               kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(embedded_sequences)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Loop to create n_conv_layers\n",
    "    \n",
    "    for n in range(n_conv_layers):\n",
    "        x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "                   kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "        \n",
    "        if n==n_conv_layers-1:\n",
    "            x = GlobalMaxPooling1D()(x)\n",
    "        else:\n",
    "            x = MaxPooling1D(filter_size)(x)\n",
    "    \n",
    "# #     ##\n",
    "#     x = Conv1D(n_filters, filter_size, activation=activation,data_format='channels_first',\n",
    "#               kernel_regularizer=l2(l2_lr), bias_regularizer=l2(l2_lr))(x) \n",
    "#     x = Dropout(dropout)(x)\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    ##\n",
    "    x = Dense(n_filters, activation=activation )(x) #128'relu'\n",
    "    preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, #'rmsprop',#adam\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "#     if show_summary:\n",
    "#     print()\n",
    "    display(model.summary())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.333479Z",
     "start_time": "2020-02-27T16:51:23.326939Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tune_clock = bs.Clock()\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "neural_network_deep = KerasClassifier(build_fn=create_deep_model,verbose=1)\n",
    "\n",
    "params_deep_model={'n_conv_layers':[3],#,5],\n",
    "                  'n_filters': [5,100],#,200],\n",
    "                 'l2_lr':[0.01],#0.005,\n",
    "                  'epochs':[10]}\n",
    "# params_deep_model={'activation': ['tanh'], \n",
    "#                    'n_conv_layers':[3,5],\n",
    "#                   'dropout': [0.2],\n",
    "#                   'epochs': [10],#,20]\n",
    "#                   'filter_size': [4],\n",
    "#                   'batch_size':[100],\n",
    "#                   'n_filters': [100,200],\n",
    "#                   'optimizer': ['adadelta'],\n",
    "#                  'l2_lr':[0.005,0.01]}\n",
    "grid = GridSearchCV(estimator=neural_network_deep,param_grid=params_deep_model)#,\n",
    "#                    n_jobs=3)\n",
    "print(params_deep_model)\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    tune_clock.tic()\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    tune_clock.toc()\n",
    "    results = grid_result.cv_results_\n",
    "    print(type(results))\n",
    "    best_deep_params = grid.best_params_\n",
    "    print(best_deep_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:18:07.481108Z",
     "start_time": "2020-02-27T16:17:58.524Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.336964Z",
     "start_time": "2020-02-27T16:51:23.334993Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ihelp(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.341309Z",
     "start_time": "2020-02-27T16:51:23.338398Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fit_model_kws={'X_train':X_train, 'y_train':y_train,\n",
    "                         'batch_size':100, 'epochs':10, 'verbose':True,\n",
    "                         'validation_data':(X_val,y_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.344652Z",
     "start_time": "2020-02-27T16:51:23.342754Z"
    }
   },
   "outputs": [],
   "source": [
    "# model, results = fit_model(create_deep_model(embedding_layer_gl,**best_deep_params),\n",
    "#                           **fit_model_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.348102Z",
     "start_time": "2020-02-27T16:51:23.346050Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ji.evaluate_classification_model(grid_result, X_train, X_test, y_train, y_test, \n",
    "#                            binary_classes=False,\n",
    "#                            conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "#                            normalize_conf_matrix=True)\n",
    "# #                                     /               save_history=True, history_filename=hist_fname,\n",
    "# #                                                    save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "# #                                                    save_summary=True,summary_filename=summary_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.351797Z",
     "start_time": "2020-02-27T16:51:23.349477Z"
    }
   },
   "outputs": [],
   "source": [
    "# cm_fname = file_dict['model_0A']['fig_conf_mat.ext']\n",
    "# hist_fname = file_dict['model_0A']['fig_keras_history.ext']\n",
    "# summary_fname = file_dict['model_0A']['model_summary']\n",
    "\n",
    "# df_class_report0A,fig0A=ji.evaluate_classification(model,history,\n",
    "#                                                    X_train, X_test,\n",
    "#                                                    y_train, y_test, \n",
    "#                                                    report_as_df=False,\n",
    "#                                                    binary_classes=False,\n",
    "#                                                    conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "#                                                    normalize_conf_matrix=True, \n",
    "#                                                    save_history=True, history_filename=hist_fname,\n",
    "#                                                    save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "#                                                    save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Dense Layers after CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:23.589973Z",
     "start_time": "2020-02-27T16:51:23.353433Z"
    }
   },
   "outputs": [],
   "source": [
    "##CNN+ MORE DENSE LAYERS\n",
    "from keras.layers import Input, Conv1D,GlobalMaxPooling1D, MaxPooling1D, Dense, Dropout,Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "print('Training model.')\n",
    "MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_gl(sequence_input)\n",
    "## Specify layer parameters\n",
    "n_units =128\n",
    "filter_size=4\n",
    "dropout =0.2\n",
    "x = Conv1D(n_units, filter_size, activation='relu')(embedded_sequences)\n",
    "x = Dropout(dropout)(x)\n",
    "x = MaxPooling1D(filter_size)(x)\n",
    "x = Conv1D(n_units, filter_size, activation='relu',data_format='channels_first')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = MaxPooling1D(filter_size)(x)\n",
    "x = Conv1D(n_units, filter_size, activation='relu')(x)\n",
    "\n",
    "x = Dropout(dropout)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "# x = Flatten()\n",
    "x = Dense(1024,activation='relu')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "x = Dense(n_units, activation='relu')(x) #128\n",
    "preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',#'rmsprop',#'adam',#\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:58.929961Z",
     "start_time": "2020-02-27T16:51:23.591957Z"
    }
   },
   "outputs": [],
   "source": [
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=256,#128,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_val,y_val))\n",
    "                    \n",
    "          #validation_split=0.1)\n",
    "clock.toc('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:01.854372Z",
     "start_time": "2020-02-27T16:51:58.931415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "df_class_report0_CNN,fig0_CNN=ji.evaluate_classification(model,history,\n",
    "                                                   X_train, X_test,\n",
    "                                                   y_train, y_test, \n",
    "#                                                          history=history,\n",
    "                                                   report_as_df=False,\n",
    "                                                   binary_classes=False,\n",
    "                                                   conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                                                   normalize_conf_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Multiple Filters into a Functional CNN Model\n",
    "- https://medium.com/datadriveninvestor/deep-learning-techniques-for-text-classification-9392ca9492c7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0A Summary\n",
    "Our model had difficulty classifying tweets by `delta_price`, but did perform better than chance (36% accuracy vs chance=33%). We will next attempt to use another type of recurrent-neural-network layer, the Gated Rectifier Unit (GRU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:01.858195Z",
     "start_time": "2020-02-27T16:52:01.855831Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## ADDING PRETRAINED GLOVE EMBEDDINGS\n",
    "# word2index, embedding_matrix = ji.load_glove_embeddings(fp,encoding='utf-8',embedding_dim=100)\n",
    "\n",
    "# vocab_size = embedding_matrix.shape[0]\n",
    "# vector_size = embedding_matrix.shape[1]\n",
    "\n",
    "# from keras import layers         \n",
    "# embedding_layer =layers.Embedding(vocab_size,#+1,\n",
    "#                                   vector_size,\n",
    "#                                   input_length=X_train.shape[1],\n",
    "#                                   weights=[embedding_matrix],\n",
    "#                                   trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:01.861832Z",
     "start_time": "2020-02-27T16:52:01.859596Z"
    }
   },
   "outputs": [],
   "source": [
    "# model0B.get_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:02.862213Z",
     "start_time": "2020-02-27T16:52:01.863219Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "model0B = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "model0B.add(embedding_layer_gl)\n",
    "\n",
    "model0B.add(layers.SpatialDropout1D(0.5))\n",
    "model0B.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model0B.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
    "model0B.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model0B.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "model0B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:20.756367Z",
     "start_time": "2020-02-27T16:52:02.863490Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "clock = bs.Clock()\n",
    "clock.tic()\n",
    "historyB = model0B.fit(X_train, y_train, epochs=num_epochs, verbose=True, validation_split=0.1,\n",
    "                     batch_size=300)#, class_weight=class_weight)#callbacks=callbacks,, validation_data=(X_val))\n",
    "clock.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:20.787550Z",
     "start_time": "2020-02-27T16:52:20.757607Z"
    }
   },
   "outputs": [],
   "source": [
    "file_dict=io.def_filename_dictionary()\n",
    "model_key = \"model_0B\" \n",
    "cm_fname = file_dict[model_key]['fig_conf_mat.ext']\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:25.710640Z",
     "start_time": "2020-02-27T16:52:20.788915Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "df_class_report0B, fig0B  = ji.evaluate_classification(model0B, historyB, \n",
    "                           X_train, X_test, y_train,y_test,report_as_df=False,\n",
    "                           conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                           binary_classes=False, normalize_conf_matrix=True, \n",
    "                           save_history=True, history_filename=hist_fname, \n",
    "                           save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "                           save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:25.714210Z",
     "start_time": "2020-02-27T16:52:25.711960Z"
    }
   },
   "outputs": [],
   "source": [
    "# save_me_as_model_0B=True\n",
    "# save_me_as_pred_nlp = True\n",
    "\n",
    "# ji.reload(ji)\n",
    "# if save_me_as_pred_nlp:\n",
    "#     model_key='nlp_model_for_predictions'\n",
    "\n",
    "# elif save_me_as_model_0B:\n",
    "#     model_key='model_0B'    \n",
    "    \n",
    "# filename = file_dict[model_key]['base_filename']\n",
    "# nlp_files = ji.save_model_weights_params(model0B,check_if_exists=True,auto_increment_name=True, \n",
    "#                                          auto_filename_suffix=True,filename_prefix=filename)\n",
    "\n",
    "# file_dict[model_key]['output_filenames'] = nlp_files\n",
    "\n",
    "# ji.update_file_directory(file_dict)\n",
    "# # ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0B Summary\n",
    "\n",
    "The GRU performed better than the LSTM model, with 39% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:26.774368Z",
     "start_time": "2020-02-27T16:52:25.715812Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.inspect_variables(locals(),show_how_to_delete=False)\n",
    "del_me= ['one_hot_results','nlp_df','text_data']#list of variable names\n",
    "for me in del_me:    \n",
    "    try: \n",
    "        exec(f'del {me}')\n",
    "        print(f'del {me} succeeded')\n",
    "    except:\n",
    "        print(f'del {me} failed')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORECASTING STOCK MARKET PRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Processing Stock Data (SCRUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:26:25.399955Z",
     "start_time": "2020-02-28T02:26:25.317788Z"
    }
   },
   "outputs": [],
   "source": [
    "# DISPLAY CODE TO BE USED BELOW TO LOAD AND PROCESS STOCK DATA\n",
    "functions_used=[ji.load_processed_stock_data, # This script combines the oriignal 4 used:\n",
    "                ji.load_raw_stock_data_from_txt,\n",
    "                ji.set_timeindex_freq,ji.custom_BH_freq,\n",
    "               ji.get_technical_indicators]\n",
    "\n",
    "ji.ihelp_menu(functions_used)\n",
    "ji.save_ihelp_menu_to_file(functions_used,'_stock_df_processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Using Price as only feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Appropriate Metrics for Time Series Forecasting\n",
    "\n",
    "- Due to the estimation of price being a precise regression, accuracy will not be an appropriate metric for judging model performance. \n",
    " - e.g. if the price was \\\\$ 114.23 and our model predicted \\\\$ 114.25, our accuracy is 0.<br><br>\n",
    "\n",
    "- **Thiel's U:**\n",
    "    - [Source of Equation/Explanation of Metric](https://docs.oracle.com/cd/E57185_01/CBREG/ch06s02s03s04.html)\n",
    "    - $ U = \\sqrt{\\frac{\\sum_{t=1 }^{n-1}\\left(\\frac{\\bar{Y}_{t+1} - Y_{t+1}}{Y_t}\\right)^2}{\\sum_{t=1 }^{n-1}\\left(\\frac{Y_{t+1} - Y_{t}}{Y_t}\\right)^2}}$\n",
    "\n",
    "\n",
    "|Thiel's U Value | Interpretation |\n",
    "| --- | --- |\n",
    "| <1 | Forecasting is better than guessing| \n",
    "| 1 | Forecasting is about as good as guessing| \n",
    "|>1 | Forecasting is worse than guessing|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:26:27.306280Z",
     "start_time": "2020-02-28T02:26:27.302358Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:27:05.187114Z",
     "start_time": "2020-02-28T02:26:28.066817Z"
    }
   },
   "outputs": [],
   "source": [
    "# fname = file_dict['stock_df']['raw_csv_file']\n",
    "\n",
    "# ji.load_stock_df_from_csv()\n",
    "reload(ji)\n",
    "folderpath='data/'\n",
    "filename = 'SP500_1min_01_23_2020.xlsx'\n",
    "raw_stock_df = ji.load_raw_stock_data_from_txt(folderpath=folderpath,filename = filename, verbose=2)\n",
    "# raw_stock_df =pd.read_excel('data/SP500_1min_01_23_2020.xlsx',parse_dates=False)\n",
    "'.xlsx' in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:27:05.192116Z",
     "start_time": "2020-02-28T02:27:05.189306Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw_stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:27:05.197429Z",
     "start_time": "2020-02-28T02:27:05.194851Z"
    }
   },
   "outputs": [],
   "source": [
    "# date_time_index = raw_stock_df[\"Date\"].astype('str') + ' '+stock_df['Time'].astype('str')\n",
    "# raw_stock_df['date_time_index'] = pd.to_datetime(date_time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:27:06.429460Z",
     "start_time": "2020-02-28T02:27:05.199784Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = ji.plotly_time_series(raw_stock_df, y_col='BidClose',as_figure=True)\n",
    "stock_df = ji.get_technical_indicators(raw_stock_df,make_price_from='BidClose')\n",
    "del raw_stock_df\n",
    "\n",
    "# SELECT DESIRED COLUMNS\n",
    "stock_df = stock_df[[\n",
    "    'price','ma7','ma21','26ema','12ema','MACD','20sd',\n",
    "    'upper_band','lower_band','ema','momentum']]\n",
    "\n",
    "# Make stock_price for twitter functions\n",
    "stock_df.dropna(inplace=True)\n",
    "ji.index_report(stock_df)\n",
    "display(stock_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T02:27:20.798436Z",
     "start_time": "2020-02-28T02:27:20.681803Z"
    }
   },
   "outputs": [],
   "source": [
    "stock_df.to_csv('data/_stock_df_with_technical_indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:53:04.472674Z",
     "start_time": "2020-02-27T16:53:04.415062Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func_list = [ji.train_test_split_by_last_days,\n",
    "           ji.make_scaler_library,\n",
    "           ji.transform_cols_from_library,\n",
    "           ji.make_train_test_series_gens]\n",
    "ihelp_menu(func_list)\n",
    "ji.save_ihelp_menu_to_file(func_list,'_stock_data_prep_for_modeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:53:04.982167Z",
     "start_time": "2020-02-27T16:53:04.473742Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "num_test_days=10\n",
    "num_train_days= 260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq( stock_df, ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df,\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:53:05.007306Z",
     "start_time": "2020-02-27T16:53:04.983404Z"
    }
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens( \n",
    "    df_train['price'], df_test['price'], \n",
    "    x_window=x_window,n_features=1,batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:53:05.533131Z",
     "start_time": "2020-02-27T16:53:05.008675Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input = x_window\n",
    "n_features = 1 # just stock Price\n",
    "\n",
    "print(f'input shape: ({n_input},{n_features})')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(units=50, input_shape =input_shape,return_sequences=True))#,kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "model1.add(LSTM(units=50, activation='relu'))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "model1.compile(loss=ji.my_rmse, metrics=['acc'],\n",
    "              optimizer=optimizers.Nadam())\n",
    "\n",
    "display(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:00.480396Z",
     "start_time": "2020-02-27T16:53:05.535014Z"
    }
   },
   "outputs": [],
   "source": [
    "## FIT MODEL\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "## set params\n",
    "epochs=5\n",
    "\n",
    "# override keras warnings\n",
    "ji.quiet_mode(True,True,True)\n",
    "\n",
    "# Instantiating clock timer\n",
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model1.fit_generator(train_generator,\n",
    "                               epochs=epochs,\n",
    "                               verbose=2, \n",
    "                               use_multiprocessing=True,\n",
    "                               workers=3)\n",
    "\n",
    "clock.toc('')\n",
    "\n",
    "\n",
    "model_key = \"model_1\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model1,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:03.888681Z",
     "start_time": "2020-02-27T16:54:00.482220Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model1 = ji.get_model_preds_df(model1, \n",
    "                                  test_generator = test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  include_train_data=True,\n",
    "                                  inverse_tf = True, \n",
    "                                  scaler = scaler_library['price'],\n",
    "                                  preds_from_gen = True, \n",
    "                                  preds_from_train_preds = True, \n",
    "                                  preds_from_test_preds = True,\n",
    "                                  iplot = True, iplot_title='Model 1: True Vs Predicted S&P 500 Price',\n",
    "                                  verbose=0)\n",
    "    \n",
    "# Get evaluation metrics\n",
    "df_results1, dfs_results1, df_shifted1 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model1['true_test_price'],\n",
    "                                   df_model1['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,4,1),\n",
    "                                   true_train_series_to_add=df_model1['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   display_U_info=True,\n",
    "                                   return_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   return_shifted_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:04.815721Z",
     "start_time": "2020-02-27T16:54:03.890000Z"
    }
   },
   "outputs": [],
   "source": [
    "save_model=True\n",
    "# ji.save_model_dfs(file_dict, 'model_1',df_model1,dfs_results1,df_shifted1)\n",
    "\n",
    "filename_prefix = file_dict['model_1']['base_filename']\n",
    "if save_model ==True:\n",
    "    model_1_output_files = ji.save_model_weights_params(model1,\n",
    "                                 filename_prefix=filename_prefix,\n",
    "                                 auto_increment_name=True,\n",
    "                                 auto_filename_suffix=True, \n",
    "                                 suffix_time_format='%m-%d-%y_%I%M%p',\n",
    "                                 save_model_layer_config_xlsx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Stock Price alone and with a prediction time-shift of -1, we achieved:\n",
    "    - $R^2$ value of 0.95\n",
    "    - RMSE value of 0.2693\n",
    "    - Thiel's $U$ value of 0.4476\n",
    "- This means our first model can explain 95% of the variance in the data ($R^2$) and that our model performed significantly better than guessing (Thiel's U value <1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Stock Price + Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXUD8NkFm5r8"
   },
   "source": [
    "### Technical Indicator Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:04.836407Z",
     "start_time": "2020-02-27T16:54:04.817132Z"
    }
   },
   "outputs": [],
   "source": [
    "# SELECT DESIRED COLUMNS\n",
    "stock_df = stock_df[[\n",
    "    'price','ma7','ma21','26ema','12ema','MACD','20sd',\n",
    "    'upper_band','lower_band','ema','momentum']]\n",
    "\n",
    "# Make stock_price for twitter functions\n",
    "stock_df.dropna(inplace=True)\n",
    "ji.index_report(stock_df)\n",
    "display(stock_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:09.577501Z",
     "start_time": "2020-02-27T16:54:04.838122Z"
    }
   },
   "outputs": [],
   "source": [
    "fig =ji.plotly_technical_indicators(stock_df,figsize=(900,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXUD8NkFm5r8"
   },
   "source": [
    "1. **7 and 21 day moving averages**\n",
    "```python\n",
    "df['ma7'] df['price'].rolling(window = 7 ).mean() #window of 7 if daily data\n",
    "df['ma21'] df['price'].rolling(window = 21).mean() #window of 21 if daily data\n",
    "```    \n",
    "2. **MACD(Moving Average Convergence Divergence)**\n",
    "\n",
    "> Moving Average Convergence Divergence (MACD) is a trend-following momentumindicator that shows the relationship between two moving averages of a security’s price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\n",
    "\n",
    ">The result of that calculation is the MACD line. A nine-day EMA of the MACD, called the \"signal line,\" is then plotted on top of the MACD line, which can function as a trigger for buy and sell signals. \n",
    "\n",
    "> Traders may buy the security when the MACD crosses above its signal line and sell - or short - the security when the MACD crosses below the signal line. Moving Average Convergence Divergence (MACD) indicators can be interpreted in several ways, but the more common methods are crossovers, divergences, and rapid rises/falls.  - _[from Investopedia](https://www.investopedia.com/terms/m/macd.asp)_\n",
    "\n",
    "```python\n",
    "df['ewma26'] = pd.ewma(df['price'], span=26)\n",
    "df['ewma12'] = pd.ewma(df['price'], span=12)\n",
    "df['MACD'] = (df['12ema']-df['26ema'])\n",
    "```\n",
    "3. **Exponentially weighted moving average**\n",
    "```python\n",
    "dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "```\n",
    "\n",
    "4. **Bollinger bands**\n",
    "    > \"Bollinger Bands® are a popular technical indicators used by traders in all markets, including stocks, futures and currencies. There are a number of uses for Bollinger Bands®, including determining overbought and oversold levels, as a trend following tool, and monitoring for breakouts. There are also some pitfalls of the indicators. In this article, we will address all these areas.\"\n",
    "> Bollinger bands are composed of three lines. One of the more common calculations of Bollinger Bands uses a 20-day simple moving average (SMA) for the middle band. The upper band is calculated by taking the middle band and adding twice the daily standard deviation, the lower band is the same but subtracts twice the daily std. - _[from Investopedia](https://www.investopedia.com/trading/using-bollinger-bands-to-gauge-trends/)_\n",
    "\n",
    "    - Boilinger Upper Band:<br>\n",
    "    $BOLU = MA(TP, n) + m * \\sigma[TP, n ]$<br><br>\n",
    "    - Boilinger Lower Band<br>\n",
    "    $ BOLD = MA(TP,n) - m * \\sigma[TP, n ]$\n",
    "    - Where:\n",
    "        - $MA$  = moving average\n",
    "        - $TP$ (typical price) = $(High + Low+Close)/ 3$\n",
    "        - $n$ is number of days in smoothing period\n",
    "        - $m$ is the number of standard deviations\n",
    "        - $\\sigma[TP, n]$ = Standard Deviations over last $n$ periods of $TP$\n",
    "\n",
    "```python\n",
    "# Create Bollinger Bands\n",
    "dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "```\n",
    "\n",
    "\n",
    "5. **Momentum**\n",
    "> \"Momentum is the rate of acceleration of a security's price or volume – that is, the speed at which the price is changing. Simply put, it refers to the rate of change on price movements for a particular asset and is usually defined as a rate. In technical analysis, momentum is considered an oscillator and is used to help identify trend lines.\" - _[from Investopedia](https://www.investopedia.com/articles/technical/081501.asp)_\n",
    "\n",
    "    - $ Momentum = V - V_x$\n",
    "    - Where:\n",
    "        - $ V $ = Latest Price\n",
    "        - $ V_x $ = Closing Price\n",
    "        - $ x $ = number of days ago\n",
    "\n",
    "```python\n",
    "# Create Momentum\n",
    "dataset['momentum'] = dataset['price']-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:10.151130Z",
     "start_time": "2020-02-27T16:54:09.578790Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "num_test_days=20\n",
    "num_train_days=260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq( stock_df, ji.custom_BH_freq() )\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df,\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:10.179333Z",
     "start_time": "2020-02-27T16:54:10.152684Z"
    }
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens( \n",
    "    df_train['price'], df_test['price'], \n",
    "    x_window=x_window,n_features=1,batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:10.185386Z",
     "start_time": "2020-02-27T16:54:10.180673Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make new time series generators with all stock_indicators for X_sequences\n",
    "train_generator, test_generator = ji.make_train_test_series_gens(\n",
    "    train_data_series=df_train,\n",
    "    test_data_series=df_test,\n",
    "    y_cols='price',\n",
    "    x_window=x_window,\n",
    "    n_features=len(df_train.columns),\n",
    "    batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:54:10.680509Z",
     "start_time": "2020-02-27T16:54:10.186886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create keras model from model_params\n",
    "import functions_combined_BEST as ji\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from IPython.display import display\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input = x_window \n",
    "n_features = len(df_train.columns) # Using stock_price and technical indicators\n",
    "\n",
    "print(f'input shape: ({n_input},{n_features}')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=50, input_shape =input_shape,return_sequences=True))#,  kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "# model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=50, activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(loss=ji.my_rmse, metrics=['acc',ji.my_rmse],\n",
    "              optimizer=optimizers.Nadam())\n",
    "\n",
    "display(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:11.398189Z",
     "start_time": "2020-02-27T16:54:10.681858Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=5\n",
    "\n",
    "clock = bs.Clock()\n",
    "print('---'*20)\n",
    "print('\\tFITTING MODEL:')\n",
    "print('---'*20,'\\n')     \n",
    "\n",
    "# start the timer\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model2.fit_generator(train_generator,epochs=epochs) \n",
    "clock.toc('')\n",
    "\n",
    "model_key = \"model_2\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model2,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:13.484351Z",
     "start_time": "2020-02-27T16:55:11.399723Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model2 = ji.get_model_preds_df(model2, \n",
    "                                  test_generator=test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  x_window=x_window,\n",
    "                                  n_features=len(df_train.columns),\n",
    "                                  scaler=scaler_library['price'],\n",
    "                                  preds_from_gen=True, \n",
    "                                  inverse_tf=True,\n",
    "                                  iplot=True,  iplot_title='Model 2: True Vs Predicted S&P 500 Price')\n",
    "\n",
    "# Compare predictions if predictions timebins shifted\n",
    "df_results2, dfs_results2, df_shifted2 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model2['true_test_price'],\n",
    "                                   df_model2['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_model2['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:13.487423Z",
     "start_time": "2020-02-27T16:55:13.485740Z"
    }
   },
   "outputs": [],
   "source": [
    "# ##SAVING DFS\n",
    "# ji.save_model_dfs(file_dict,'model_2',\n",
    "#                df_model=df_model2,\n",
    "#               df_results=dfs_results2,\n",
    "#               df_shifted=df_shifted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:13.761292Z",
     "start_time": "2020-02-27T16:55:13.488496Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results2, dfs_results2, df_shifted2 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model2['true_test_price'],\n",
    "                                   df_model2['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_model2['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Stock Price plus technical indicators, and with a prediction time-shift of -1, we achieved:\n",
    "    - $R^2$ value of 0.98\n",
    "    - RMSE value of 0.269\n",
    "    - Thiel's $U$ value of 0.4812\n",
    "- This means our second model can explain \\_\\_% of the variance in the data ($R^2$) and that our model performed ~~significantly better~~ than guessing (Thiel's U value ~~<1.0~~)\n",
    "\n",
    "- It is surprisng that the model's performance is so poor by adding the technical indicators.\n",
    "    - Further investigation is required to understand why the $R^2$ value is negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINING TWEET STATS, NLP CLASSIFICATION, AND MARKET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:13.773507Z",
     "start_time": "2020-02-27T16:55:13.762480Z"
    }
   },
   "outputs": [],
   "source": [
    "# # LOAD IN FULL STOCK DATASET using ClosingBig S&P500 WITH INDEX.FREQ=CBH\n",
    "# # fname = file_dict['stock_df']['stock_df_with_indicators']\n",
    "# full_df = ji.load_processed_stock_data(processed_data_filename=fname)\n",
    "\n",
    "# # SELECT DESIRED COLUMNS\n",
    "# stock_df = full_df[[\n",
    "#     'price','ma7','ma21','26ema','12ema','MACD',\n",
    "#     '20sd','upper_band','lower_band','ema','momentum'\n",
    "# ]]\n",
    "\n",
    "# stock_df.head()\n",
    "\n",
    "# stock_df['date_time'] = stock_df.index.to_series()\n",
    "# ji.index_report(stock_df)\n",
    "\n",
    "# stock_df.sort_index(inplace=True)\n",
    "# display(stock_df.head(2),stock_df.tail(2))\n",
    "# del full_df\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOKMARK 022720- Error merging dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:56:49.412317Z",
     "start_time": "2020-02-27T16:56:49.201997Z"
    }
   },
   "outputs": [],
   "source": [
    "## LOAD IN RAW TWITTER DATA, NO PROCESSING\n",
    "# from functions_combined_BEST import ihelp_menu2\n",
    "# # file_dict = ji.load_filename_directory()\n",
    "raw_tweets = 'data/trump_tweets_12012016_to_01012020.csv'\n",
    "sp500 = 'data/SP500_1min_01_23_2020.xlsx'\n",
    "# dft = pd.read_csv(tweets)\n",
    "# dfs = pd.read_excel(sp500)\n",
    "np.random.seed(42)\n",
    "\n",
    "twitter_df= ji.load_raw_twitter_file(filename= raw_tweets,#'data/trump_tweets_12012016_to_01012020.csv'\n",
    "\n",
    "                                     date_as_index=True,\n",
    "                                     rename_map={'text': 'content', 'created_at': 'date_time_index'})\n",
    "twitter_df = ji.check_twitter_df(twitter_df,text_col='content',remove_duplicates=True, remove_long_strings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:56:50.399900Z",
     "start_time": "2020-02-27T16:56:50.265507Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAKE TIME INTERVALS BASED ON BUSINESS HOUR START (09:30-10:30)\n",
    "time_intervals= \\\n",
    "ji.make_time_index_intervals(stock_df.reset_index(drop=True),\n",
    "                             col='date_time_index', \n",
    "                             closed='right',\n",
    "                             return_interval_dicts=False) \n",
    "time_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.568586Z",
     "start_time": "2020-02-27T16:45:27.429Z"
    }
   },
   "outputs": [],
   "source": [
    "## USE THE TIME INDEX TO FILTER OUT TWEETS FROM THE HOUR PRIOR\n",
    "twitter_df, bin_codes = ji.bin_df_by_date_intervals(twitter_df ,time_intervals)\n",
    "stock_df, bin_codes_stock = ji.bin_df_by_date_intervals(stock_df.reset_index(drop=True), time_intervals, column='date_time_index')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.569549Z",
     "start_time": "2020-02-27T16:45:27.434Z"
    }
   },
   "outputs": [],
   "source": [
    "## COLLAPSE DFs BY CODED BINS\n",
    "twitter_grouped = ji.collapse_df_by_group_index_col(twitter_df,\n",
    "                                                    group_index_col='int_bins',\n",
    "                                                    drop_orig=True,\n",
    "                                                    verbose=0)\n",
    "\n",
    "stocks_grouped = ji.collapse_df_by_group_index_col(stock_df,\n",
    "                                                    drop_orig=True,\n",
    "                                                    group_index_col='int_bins', \n",
    "                                                  verbose=0)\n",
    "display(twitter_grouped.head(2),stocks_grouped.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.570390Z",
     "start_time": "2020-02-27T16:45:27.439Z"
    }
   },
   "outputs": [],
   "source": [
    "# 022720\n",
    "stocks_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.571743Z",
     "start_time": "2020-02-27T16:45:27.444Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu(ji.merge_stocks_and_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.572766Z",
     "start_time": "2020-02-27T16:45:27.450Z"
    }
   },
   "outputs": [],
   "source": [
    "## STOCKS AND TWEETS \n",
    "stocks_grouped['date_time'] = stocks_grouped.index.to_series()\n",
    "df_combined = ji.merge_stocks_and_tweets(stocks_grouped, \n",
    "                                      twitter_grouped,\n",
    "                                      on='int_bins',how='left',\n",
    "                                      show_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.573758Z",
     "start_time": "2020-02-27T16:45:27.455Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.column_report(df_combined, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.574897Z",
     "start_time": "2020-02-27T16:45:27.460Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check for and address new null values\n",
    "ji.check_null_small(df_combined);\n",
    "cols_to_fill_zeros = ['num_tweets','total_retweet_count','total_favorite_count']\n",
    "for col in cols_to_fill_zeros:\n",
    "    idx_null = ji.find_null_idx(df_combined, column=col)\n",
    "    df_combined.loc[idx_null,col] = 0\n",
    "\n",
    "cols_to_fill_blank_str = ['group_content','source','tweet_times','is_retweet']\n",
    "for col in cols_to_fill_blank_str:\n",
    "    idx_null = ji.find_null_idx(df_combined, column=col)\n",
    "    df_combined.loc[idx_null, col] = \"\"\n",
    "ji.check_null_small(df_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.575899Z",
     "start_time": "2020-02-27T16:45:27.465Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = file_dict['df_combined']['pre_nlp']\n",
    "df_combined.to_csv(fname)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.576868Z",
     "start_time": "2020-02-27T16:45:27.470Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add nlp\n",
    "df_nlp = ji.full_twitter_df_processing(df_combined,'group_content',force=True)\n",
    "ji.column_report(df_nlp, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.577979Z",
     "start_time": "2020-02-27T16:45:27.475Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use case ratio null values as index to replace values\n",
    "idx_null= ji.check_null_small(df_nlp,null_index_column='case_ratio')\n",
    "df_nlp.loc[idx_null,'case_ratio'] = 0.0\n",
    "ji.check_null_small(df_nlp)\n",
    "\n",
    "## replace sentiment_class, set =-1\n",
    "cols_to_replace_misleading_values = ['sentiment_class']\n",
    "for col in cols_to_replace_misleading_values:\n",
    "    df_nlp.loc[idx_null,col] = -1\n",
    "\n",
    "## remap sentiment class\n",
    "sent_class_mapper = {'neg':0, -1:1, 'pos':2}\n",
    "df_nlp['sentiment_class'] = df_nlp['sentiment_class'].apply(lambda x: sent_class_mapper[x])\n",
    "\n",
    "bool_cols_to_ints = ['has_tweets']\n",
    "for col in bool_cols_to_ints:\n",
    "    df_nlp[col] = df_nlp[col].apply(lambda x: 1 if x==True else 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.579079Z",
     "start_time": "2020-02-27T16:45:27.481Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nlp#.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.580279Z",
     "start_time": "2020-02-27T16:45:27.486Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.display_same_tweet_diff_cols(df_nlp.groupby('has_tweets').get_group(True),\n",
    "                                columns=['group_content','content_min_clean',\n",
    "                                         'cleaned_stopped_lemmas'],as_md=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.581303Z",
     "start_time": "2020-02-27T16:45:27.493Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_twitter_df(df_nlp,char_limit=61*350)\n",
    "# get_floats = df_nlp['content_min_clean'].apply(lambda x: isinstance(x,float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.582508Z",
     "start_time": "2020-02-27T16:45:27.499Z"
    }
   },
   "outputs": [],
   "source": [
    "fname =file_dict['df_combined']['post_nlp']\n",
    "df_nlp.to_csv(fname)\n",
    "# print(f'saved to {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in NLP Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.583690Z",
     "start_time": "2020-02-27T16:45:27.507Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_recent_filenames(full_filename,str_to_find=None):\n",
    "    import os\n",
    "    import time\n",
    "    fparts = full_filename.split('/')\n",
    "    folder = '/'.join(fparts[0:-1])\n",
    "    name = fparts[-1]\n",
    "    \n",
    "    filelist = os.listdir(folder)\n",
    "\n",
    "    mtimes = [['file','date modified']]\n",
    "    for file in filelist:\n",
    "        if str_to_find is None:\n",
    "            mtimes.append([file, time.ctime(os.path.getmtime(folder+'/'+file))])\n",
    "        elif str_to_find in file:\n",
    "            mtimes.append([file, time.ctime(os.path.getmtime(folder+'/'+file))])\n",
    "    res = bs.list2df(mtimes)\n",
    "    res['date modified'] = pd.to_datetime(res['date modified'])\n",
    "    res.set_index('date modified',inplace=True)\n",
    "    res.sort_index(ascending=False, inplace=True)\n",
    "    \n",
    "    most_recent = res.iloc[0]\n",
    "    import re\n",
    "    re.compile(r'()')\n",
    "    \n",
    "    return    res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.584738Z",
     "start_time": "2020-02-27T16:45:27.512Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load the nlp model and weights with layers set trainable=False\n",
    "base_fname = file_dict['nlp_model_for_predictions']['base_filename']\n",
    "nlp_model,df_model_layers =  ji.load_model_weights_params(base_filename= base_fname,#'models/NLP/nlp_model0B__09-02-2019_0121pm',\n",
    "                                        load_model_params=False,\n",
    "                                        load_model_layers_excel=True,\n",
    "                                        trainable=False)\n",
    "## Load in Word2Vec model from earlier\n",
    "w2v_model = io.load_word2vec(file_dict=file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions for Hour-Binned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.585827Z",
     "start_time": "2020-02-27T16:45:27.519Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu([ji.get_tokenizer_and_text_sequences,\n",
    "           ji.replace_embedding_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.586876Z",
     "start_time": "2020-02-27T16:45:27.524Z"
    }
   },
   "outputs": [],
   "source": [
    "## GET X_SEQUENES FOR BINNED TWEETS AND CREATE NEW EMBEDDING LAYER FOR THEIR SIZE\n",
    "text_data=df_nlp['cleaned_stopped_lemmas']\n",
    "tokenizer, X_sequences = ji.get_tokenizer_and_text_sequences(w2v_model,text_data)\n",
    "\n",
    "new_nlp_model = ji.replace_embedding_layer(nlp_model,w2v_model,text_data,verbose=2)\n",
    "new_nlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.587833Z",
     "start_time": "2020-02-27T16:45:27.529Z"
    }
   },
   "outputs": [],
   "source": [
    "## GET PREDICTIONS FROM NEW MODEL\n",
    "preds = new_nlp_model.predict_classes(X_sequences)\n",
    "print(type(preds), preds.shape)\n",
    "ji.check_y_class_balance(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.589066Z",
     "start_time": "2020-02-27T16:45:27.535Z"
    }
   },
   "outputs": [],
   "source": [
    "## add to df\n",
    "df_nlp['pred_classes_int'] = preds\n",
    "mapper= {0:'neg',  1:'no_change', 2:'pos'}\n",
    "df_nlp['pred_classes'] = df_nlp['pred_classes_int'].apply(lambda x: mapper[x])\n",
    "display(df_nlp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Stock Price + Indicators + NLP Preds & Tweet Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize colums for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.590182Z",
     "start_time": "2020-02-27T16:45:27.544Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined = df_nlp\n",
    "\n",
    "model_col_list = ['price', 'ma7', 'ma21', '26ema', '12ema', 'MACD', '20sd', 'upper_band','lower_band', 'ema', 'momentum',\n",
    "                  'has_tweets','num_tweets','case_ratio', 'compound_score','pos','neu','neg','sentiment_class',\n",
    "                  'pred_classes','pred_classes_int','total_favorite_count','total_retweet_count']\n",
    "\n",
    "df_combined = ji.set_timeindex_freq(df_combined,fill_nulls=False)\n",
    "\n",
    "df_to_model = df_combined[model_col_list].copy()\n",
    "df_to_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.591194Z",
     "start_time": "2020-02-27T16:45:27.549Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "num_test_days=20\n",
    "num_train_days=260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=2\n",
    "\n",
    "cols_to_exclude = ['pred_classes','has_tweets']\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(df_to_model.drop(cols_to_exclude,axis=1), ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(df_to_model.drop(cols_to_exclude,axis=1),\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.592303Z",
     "start_time": "2020-02-27T16:45:27.554Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens(\n",
    "    train_data_series=df_train,\n",
    "    test_data_series=df_test,\n",
    "    y_cols='price',\n",
    "    x_window=x_window,\n",
    "    n_features=len(df_train.columns),\n",
    "    batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.593295Z",
     "start_time": "2020-02-27T16:45:27.560Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from IPython.display import display\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input =x_window\n",
    "n_features = len(df_train.columns)\n",
    "print(f'input shape: ({n_input},{n_features})')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(units=100, input_shape =input_shape,return_sequences=True,dropout=0.3,recurrent_dropout=0.3))#,  kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "model3.add(LSTM(units=100, activation='relu', return_sequences=False,dropout=0.3,recurrent_dropout=0.3))\n",
    "model3.add(Dense(1))\n",
    "\n",
    "model3.compile(loss=ji.my_rmse, metrics=['acc'],optimizer=optimizers.Nadam())\n",
    "    \n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.594272Z",
     "start_time": "2020-02-27T16:45:27.565Z"
    }
   },
   "outputs": [],
   "source": [
    "## FIT MODEL\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "## set params\n",
    "epochs=5\n",
    "\n",
    "# override keras warnings\n",
    "ji.quiet_mode(True,True,True)\n",
    "\n",
    "# Instantiating clock timer\n",
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model3.fit_generator(train_generator,\n",
    "                               epochs=epochs,\n",
    "                               verbose=2, \n",
    "                               use_multiprocessing=True,\n",
    "                               workers=3)\n",
    "clock.toc('')\n",
    "\n",
    "model_key = \"model_3\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model3,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.595439Z",
     "start_time": "2020-02-27T16:45:27.572Z"
    }
   },
   "outputs": [],
   "source": [
    "### PREFER NEW WAY - GET DF_MODEL FIRST THEN GET EVALUATE_REGRESSION INFORMATION?\n",
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model3 = ji.get_model_preds_df(model3, \n",
    "                                  test_generator = test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  include_train_data=True,\n",
    "                                  inverse_tf = True, \n",
    "                                  scaler = scaler_library['price'],\n",
    "                                  preds_from_gen = True, \n",
    "                                  iplot = False,\n",
    "                                  verbose=1)\n",
    "\n",
    "ji.plotly_true_vs_preds_subplots(df_model3,title='Model 3: True Vs Predicted S&P 500 Price')\n",
    "    \n",
    "# Get evaluation metrics\n",
    "df_results3, dfs_results3, df_shifted3 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model3['true_test_price'],\n",
    "                                   df_model3['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,4,1),\n",
    "                                   true_train_series_to_add=df_model3['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   display_U_info=True,\n",
    "                                   return_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   return_shifted_df=True)\n",
    "\n",
    "\n",
    "save_model=True\n",
    "ji.save_model_dfs(file_dict, 'model_3',df_model3,dfs_results3,df_shifted3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.596515Z",
     "start_time": "2020-02-27T16:45:27.578Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_prefix = file_dict['model_3']['base_filename']\n",
    "if save_model ==True:\n",
    "    model_3_output_files = bs.save_model_weights_params(model3,\n",
    "                                 filename_prefix=filename_prefix,\n",
    "                                 auto_increment_name=True,\n",
    "                                 auto_filename_suffix=True, \n",
    "                                 suffix_time_format='%m-%d-%y_%I%M%p',\n",
    "                                 save_model_layer_config_xlsx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Stock Price plus technical indicators, and with a prediction time-shift of -1, we achieved:\n",
    "    - $R^2$ value of \n",
    "    - RMSE value of \n",
    "    - Thiel's $U$ value of \n",
    "- This means our second model can explain \\_\\_% of the variance in the data ($R^2$) and that our model performed ~~significantly better~~ than guessing (Thiel's U value ~~<1.0~~)\n",
    "\n",
    "- It is surprisng that the model's performance is so poor by adding the technical indicators.\n",
    "    - Further investigation is required to understand why the $R^2$ value is negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model X: XGB Regression + Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.597543Z",
     "start_time": "2020-02-27T16:45:27.588Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "reload(ji)\n",
    "num_test_days=20\n",
    "num_train_days=2*52*5\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "cols_to_exclude = ['pred_classes','has_tweets']\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(df_to_model.drop(cols_to_exclude,axis=1), ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(df_to_model.drop(cols_to_exclude,axis=1),\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)\n",
    "\n",
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.598658Z",
     "start_time": "2020-02-27T16:45:27.592Z"
    }
   },
   "outputs": [],
   "source": [
    "## Shift price values such that the y-value being predicted is the following hour's Closing Price\n",
    "df_train['price_shifted'] = df_train['price'].shift(-1)\n",
    "df_test['price_shifted'] = df_test['price'].shift(-1)\n",
    "\n",
    "display(df_train[['price','price_shifted','momentum','ema','num_tweets',]].head(10))\n",
    "\n",
    "# Drop the couple of null values created by the shift\n",
    "df_train.dropna(subset=['price_shifted'], inplace=True)\n",
    "df_test.dropna(subset=['price_shifted'], inplace=True)\n",
    "\n",
    "## Drop columns and make train-test-X and y\n",
    "target_col = 'price_shifted'\n",
    "drop_cols = ['price_shifted','price']\n",
    "\n",
    "X_train = df_train.drop(drop_cols,axis=1)\n",
    "y_train = df_train[target_col]\n",
    "X_test = df_test.drop(drop_cols,axis=1)\n",
    "y_test = df_test[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.599737Z",
     "start_time": "2020-02-27T16:45:27.597Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "reg = xgb.XGBRegressor(n_estimators=1000,silent=False,max_depth=4)\n",
    "\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)\n",
    "\n",
    "\n",
    "## Get Predictions\n",
    "pred_price = reg.predict(X_test)\n",
    "pred_price_series = pd.Series(pred_price,index=df_test.index,name='pred_test_price')#.plot()\n",
    "df_xgb = pd.concat([df_train['price'].rename('true_train_price'), pred_price_series,df_test['price'].rename('true_test_price')],axis=1)\n",
    "\n",
    "\n",
    "df_results = ji.evaluate_regression(df_test['price'], pred_price_series,show_results=True);\n",
    "\n",
    "clock.toc('')\n",
    "fig = ji.plotly_true_vs_preds_subplots(df_xgb,true_train_col='true_train_price',\n",
    "                                true_test_col='true_test_price',\n",
    "                                pred_test_columns='pred_test_price',\n",
    "                                      title='Model X: True Vs Predicted S&P 500 Price')\n",
    "\n",
    "\n",
    "## PLOT FEATURE IMPORTANCE\n",
    "feature_importance={}\n",
    "for import_type in ['weight','gain','cover']:\n",
    "    reg.importance_type = import_type\n",
    "    cur_importances = reg.feature_importances_\n",
    "    feature_importance[import_type] = pd.Series(data = cur_importances,\n",
    "                                               index=df_train.drop(drop_cols,axis=1).columns,\n",
    "                                               name=import_type)\n",
    "\n",
    "df_importance = pd.DataFrame(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.600890Z",
     "start_time": "2020-02-27T16:45:27.601Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "importance_fig = df_importance.sort_values(by='weight', ascending=True).iplot(kind='barh',theme='solar',\n",
    "                                                                    title='Feature Importance',\n",
    "                                                                    xTitle='Relative Importance<br>(sum=1.0)',\n",
    "                                                                    asFigure=True,dimensions=(1000, 600))\n",
    "\n",
    "iplot(importance_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.601793Z",
     "start_time": "2020-02-27T16:45:27.606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ihelp(df_importance.iplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.602901Z",
     "start_time": "2020-02-27T16:45:27.611Z"
    }
   },
   "outputs": [],
   "source": [
    "  \n",
    "importance_fig = df_importance.sort_values(by='weight', ascending=True).iplot(kind='barh',theme='solar',\n",
    "                                                                    title='Feature Importance',\n",
    "                                                                    xTitle='Relative Importance<br>(sum=1.0)',\n",
    "                                                                    asFigure=True)\n",
    "\n",
    "iplot(importance_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.604017Z",
     "start_time": "2020-02-27T16:45:27.616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare predictions if predictions timebins shifted\n",
    "df_resultsX, dfs_resultsX, df_shiftedX =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_xgb['true_test_price'],\n",
    "                                   df_xgb['pred_test_price'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_xgb['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)\n",
    "df_importance.to_csv('results/modelxgb/df_importance.csv')\n",
    "\n",
    "ji.save_model_dfs(file_dict, 'model_xgb',df_xgb,dfs_resultsX,df_shiftedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.605096Z",
     "start_time": "2020-02-27T16:45:27.620Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_vis = xgb.to_graphviz(reg)\n",
    "tree_vis.render(\"xgb_full_model_\",format=\"png\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model X Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBRegressor blows all neural networks out of the water, without requiring a time-shift.**\n",
    "- Using Stock Price plus technical indicators, and with a prediction time-shift of -1, we achieved:\n",
    "    - $R^2$ value of 0.99\n",
    "    - RMSE value of 0.0077\n",
    "    - Thiel's $U$ value of 0.33\n",
    "    \n",
    "    0.0077\t0.3254\n",
    "- This means our second model can explain 98% of the variance in the data ($R^2$) and that our model performed remarkably  better than guessing (Thiel's U value).\n",
    "\n",
    "- It is surprisng how good the XGB results are in comparison to prior models. \n",
    "    - The model is extremely fast and very accurate.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:55:16.606043Z",
     "start_time": "2020-02-27T16:45:27.632Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs_list = {'Model 1':dfs_results1,\n",
    "            'Model 2':dfs_results2,\n",
    "            'Model 3':dfs_results3,\n",
    "            'XGB Regressor':dfs_resultsX}\n",
    "for k,v in dfs_list.items():\n",
    "    new_cap = f'Evaluation Metrics for {k}'\n",
    "    display(v.set_caption(new_cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xNf73gC9daS-",
    "eE3D-Avztybj"
   ],
   "name": "Capstone Project Outline + Analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278.688px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "25"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 265.340454,
   "position": {
    "height": "40px",
    "left": "751.591px",
    "right": "20px",
    "top": "49px",
    "width": "607.773px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
